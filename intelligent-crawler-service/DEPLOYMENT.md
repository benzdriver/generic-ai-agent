# ğŸš€ Intelligent Crawler Service éƒ¨ç½²æŒ‡å—

## ğŸ“‹ å‰ç½®è¦æ±‚

- Docker 20.10+
- Docker Compose 2.0+
- OpenAI API Key æˆ– Anthropic API Key
- è‡³å°‘ 4GB å¯ç”¨å†…å­˜

## ğŸ”§ å¿«é€Ÿéƒ¨ç½²

### 1. å‡†å¤‡ç¯å¢ƒ

```bash
# å…‹éš†æˆ–å¤åˆ¶æœåŠ¡ç›®å½•
cd intelligent-crawler-service

# å¤åˆ¶ç¯å¢ƒå˜é‡æ–‡ä»¶
cp .env.example .env

# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œæ·»åŠ ä½ çš„ API Keys
# å¿…é¡»è®¾ç½®ï¼šOPENAI_API_KEY æˆ– ANTHROPIC_API_KEY
nano .env
```

### 2. æ„å»ºå’Œå¯åŠ¨æœåŠ¡

```bash
# æ„å»º Docker é•œåƒ
docker-compose build

# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f
```

### 3. éªŒè¯æœåŠ¡

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8080/health

# API æ–‡æ¡£
open http://localhost:8080/docs

# Flower (ä»»åŠ¡ç›‘æ§)
open http://localhost:5555
```

## ğŸ“¡ ä½¿ç”¨ç¤ºä¾‹

### åˆ›å»ºçˆ¬å–ä»»åŠ¡

```bash
curl -X POST http://localhost:8080/api/v1/crawl \
  -H "Content-Type: application/json" \
  -d '{
    "urls": ["https://www.canada.ca/en/immigration-refugees-citizenship.html"],
    "config": {
      "max_depth": 3,
      "ai_evaluation": true,
      "min_quality_score": 0.6,
      "extract_tables": true
    },
    "collection_name": "immigration_docs"
  }'
```

### æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

```bash
# æ›¿æ¢ {job_id} ä¸ºå®é™…çš„ä»»åŠ¡ ID
curl http://localhost:8080/api/v1/crawl/{job_id}
```

### æœç´¢çŸ¥è¯†åº“

```bash
curl -X POST http://localhost:8080/api/v1/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Express Entry requirements",
    "collection": "immigration_docs",
    "top_k": 5
  }'
```

## ğŸ” æœåŠ¡æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API       â”‚â”€â”€â”€â”€â–¶â”‚   Redis      â”‚â”€â”€â”€â”€â–¶â”‚  Workers    â”‚
â”‚  (Port 8080)â”‚     â”‚ (Task Queue) â”‚     â”‚  (Celery)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                         â”‚
       â–¼                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL â”‚                          â”‚   Qdrant    â”‚
â”‚  (Metadata) â”‚                          â”‚  (Vectors)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š ç›‘æ§å’Œç®¡ç†

### æŸ¥çœ‹ä»»åŠ¡é˜Ÿåˆ—

```bash
# è®¿é—® Flower Web UI
open http://localhost:5555

# æˆ–ä½¿ç”¨ Redis CLI
docker-compose exec redis redis-cli
> LLEN celery
```

### æ•°æ®åº“ç®¡ç†

```bash
# è¿æ¥åˆ° PostgreSQL
docker-compose exec postgres psql -U crawler -d crawler_db

# æŸ¥çœ‹çˆ¬å–ä»»åŠ¡
SELECT * FROM crawl_jobs ORDER BY created_at DESC LIMIT 10;

# æŸ¥çœ‹å·²çˆ¬å–é¡µé¢
SELECT url, quality_score, crawled_at FROM crawled_pages 
WHERE quality_score > 0.7 
ORDER BY crawled_at DESC LIMIT 20;
```

### å‘é‡æ•°æ®åº“ç®¡ç†

```bash
# Qdrant Web UI
open http://localhost:6333/dashboard

# æŸ¥çœ‹é›†åˆ
curl http://localhost:6333/collections
```

## ğŸ”§ é…ç½®ä¼˜åŒ–

### æ€§èƒ½è°ƒä¼˜

ç¼–è¾‘ `docker-compose.yml`:

```yaml
services:
  crawler-worker:
    deploy:
      replicas: 5  # å¢åŠ çˆ¬è™«å·¥ä½œå™¨æ•°é‡
    environment:
      - BROWSER_POOL_SIZE=10  # å¢åŠ æµè§ˆå™¨æ± å¤§å°

  ai-worker:
    deploy:
      replicas: 3  # å¢åŠ  AI å·¥ä½œå™¨æ•°é‡
```

### èµ„æºé™åˆ¶

```yaml
services:
  api:
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          memory: 512M
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **API æ— æ³•å¯åŠ¨**
   ```bash
   # æ£€æŸ¥ç«¯å£å ç”¨
   lsof -i :8080
   
   # æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
   docker-compose logs api
   ```

2. **çˆ¬è™«ä»»åŠ¡å¤±è´¥**
   ```bash
   # æ£€æŸ¥ worker æ—¥å¿—
   docker-compose logs crawler-worker
   
   # é‡å¯ workers
   docker-compose restart crawler-worker
   ```

3. **å†…å­˜ä¸è¶³**
   ```bash
   # å‡å°‘å¹¶å‘æ•°
   # ç¼–è¾‘ .env
   MAX_CONCURRENT_CRAWLS=5
   BROWSER_POOL_SIZE=3
   ```

## ğŸ”’ å®‰å…¨å»ºè®®

1. **ç”Ÿäº§ç¯å¢ƒé…ç½®**
   - ä½¿ç”¨å¼ºå¯†ç æ›¿æ¢é»˜è®¤çš„æ•°æ®åº“å¯†ç 
   - å¯ç”¨ API è®¤è¯
   - é…ç½® HTTPS

2. **å¤‡ä»½ç­–ç•¥**
   ```bash
   # å¤‡ä»½ PostgreSQL
   docker-compose exec postgres pg_dump -U crawler crawler_db > backup.sql
   
   # å¤‡ä»½ Qdrant
   docker-compose exec qdrant qdrant-backup /qdrant/storage /backup
   ```

## ğŸ“ˆ æ‰©å±•éƒ¨ç½²

### Kubernetes éƒ¨ç½²

æŸ¥çœ‹ `k8s/` ç›®å½•ä¸­çš„ Kubernetes é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦å¯ä»¥åˆ›å»ºï¼‰ã€‚

### äº‘æœåŠ¡éƒ¨ç½²

- **AWS**: ä½¿ç”¨ ECS æˆ– EKS
- **GCP**: ä½¿ç”¨ Cloud Run æˆ– GKE
- **Azure**: ä½¿ç”¨ Container Instances æˆ– AKS

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ï¼š
- API æ–‡æ¡£: http://localhost:8080/docs
- é¡¹ç›® README: [README.md](README.md)
- æ¼”ç¤ºè„šæœ¬: `python demo.py`

---

**æç¤º**: é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½ Docker é•œåƒå’Œå®‰è£…ä¾èµ–ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚