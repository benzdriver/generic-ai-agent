#!/usr/bin/env python3
"""
é€šç”¨çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ (Generic Knowledge Management System)
æ”¯æŒä»»ä½•é¢†åŸŸçš„çŸ¥è¯†åº“æ„å»ºå’Œç®¡ç†
é€šè¿‡YAMLé…ç½®é©±åŠ¨ï¼Œå®ç°çœŸæ­£çš„é¢†åŸŸæ— å…³æ€§
"""

import os
import sys
import yaml
import json
import time
import hashlib
import asyncio
import argparse
from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional, Type, Tuple
from dataclasses import dataclass, asdict
import importlib.util
import logging
import uuid # å¯¼å…¥uuidåº“
import re # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“
from collections import Counter # å¯¼å…¥Counter
import numpy as np # Import numpy for cosine similarity
import shutil # Import shutil for file operations
from urllib.parse import urlparse, urljoin
# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.infrastructure.vector_store.factory import VectorStoreFactory
from src.infrastructure.vector_store.embedding_router import get_embedding

def get_embedding_batch(texts: List[str]) -> List[List[float]]:
    """æ‰¹é‡è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
    try:
        # é€ä¸ªå¤„ç†ï¼Œå› ä¸ºembedding_routerå¯èƒ½ä¸æ”¯æŒæ‰¹é‡
        return [get_embedding(text) for text in texts if text]
    except Exception as e:
        logger.error(f"æ‰¹é‡è·å–embeddingå¤±è´¥: {e}")
        return [[] for _ in texts]

def cosine_similarity(v1: List[float], v2: List[float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    if not v1 or not v2:
        return 0.0
    v1_np = np.array(v1)
    v2_np = np.array(v2)
    dot_product = np.dot(v1_np, v2_np)
    norm_v1 = np.linalg.norm(v1_np)
    norm_v2 = np.linalg.norm(v2_np)
    
    if norm_v1 == 0 or norm_v2 == 0:
        return 0.0
        
    return dot_product / (norm_v1 * norm_v2)

try:
    from playwright.async_api import async_playwright, Browser, Page
    HAS_PLAYWRIGHT = True
except ImportError:
    HAS_PLAYWRIGHT = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class DomainConfig:
    """é¢†åŸŸé…ç½®"""
    name: str
    description: str
    collection_name: str
    sources: List[Dict[str, Any]]
    parser_class: Optional[str] = None
    quality_rules: Optional[Dict[str, Any]] = None
    test_queries: Optional[List[Dict[str, Any]]] = None
    update_frequency_days: int = 7
    enabled: bool = True

@dataclass
class SourceConfig:
    """æ•°æ®æºé…ç½®"""
    name: str
    url: str
    type: str  # website, api, file, database
    parser_config: Optional[Dict[str, Any]] = None
    selectors: Optional[Dict[str, str]] = None
    priority: int = 1
    last_updated: Optional[str] = None
    content_hash: Optional[str] = None

class ContentParser(ABC):
    """å†…å®¹è§£æå™¨åŸºç±»"""
    
    @abstractmethod
    def parse(self, content: str, config: Dict[str, Any]) -> List[str]:
        """è§£æå†…å®¹ï¼Œè¿”å›æ®µè½åˆ—è¡¨"""
        pass
    
    @abstractmethod
    def extract_metadata(self, content: str, url: str) -> Dict[str, Any]:
        """æå–å…ƒæ•°æ®"""
        pass

class GenericHTMLParser(ContentParser):
    """é€šç”¨HTMLè§£æå™¨"""
    
    def parse(self, content: str, config: Dict[str, Any]) -> List[str]:
        """é€šç”¨HTMLè§£æ"""
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(content, 'html.parser')
        
        # ä½¿ç”¨é…ç½®ä¸­çš„é€‰æ‹©å™¨
        selectors = config.get('selectors', {})
        
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        exclude_selectors = selectors.get('exclude', 'script, style, nav, footer, header')
        for element in soup.select(exclude_selectors):
            element.decompose()
        
        # æå–å†…å®¹
        content_selector = selectors.get('content', 'p, div.content, article, section')
        paragraphs = []
        
        for element in soup.select(content_selector):
            text = element.get_text(strip=True)
            min_length = config.get('min_paragraph_length', 50)
            if len(text) >= min_length:
                paragraphs.append(text)
        
        max_paragraphs = config.get('max_paragraphs', 50)
        return paragraphs[:max_paragraphs]
    
    def extract_metadata(self, content: str, url: str) -> Dict[str, Any]:
        """æå–é€šç”¨å…ƒæ•°æ®"""
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(content, 'html.parser')
        
        metadata = {
            'url': url,
            'scraped_at': datetime.now().isoformat()
        }
        
        # å°è¯•æå–æ ‡é¢˜
        title_tag = soup.find('title')
        if title_tag:
            metadata['title'] = title_tag.get_text(strip=True)
        
        # å°è¯•æå–æè¿°
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            metadata['description'] = meta_desc.get('content', '')
        
        # å°è¯•æå–å…³é”®è¯
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if meta_keywords:
            metadata['keywords'] = [k.strip() for k in meta_keywords.get('content', '').split(',')]
        
        return metadata

class QualityChecker:
    """å†…å®¹è´¨é‡æ£€æŸ¥å™¨"""
    
    def __init__(self, rules: Dict[str, Any]):
        self.rules = self._default_rules()
        if rules:
            self.rules.update(rules)
    
    def _default_rules(self) -> Dict[str, Any]:
        """é»˜è®¤è´¨é‡è§„åˆ™"""
        return {
            'min_length': 10,
            'max_length': 5000,
            'min_unique_words': 5,
            'max_duplicate_ratio': 0.3,
            'forbidden_patterns': [],
            'required_patterns': [],
            'enable_semantic_check': False,
            'semantic_relevance_threshold': 0.4,
            'required_patterns': [],
            'forbidden_patterns': []
        }
    
    def _check_basic_rules(self, text: str) -> Tuple[bool, float, str]:
        """æ‰§è¡ŒåŸºæœ¬çš„ã€ä¸ä¾èµ–AIçš„è§„åˆ™æ£€æŸ¥"""
        # Length check
        if not (self.rules['min_length'] <= len(text) <= self.rules['max_length']):
            reason = f"é•¿åº¦ä¸ç¬¦ (is {len(text)}, should be {self.rules['min_length']}-{self.rules['max_length']})"
            return False, 0.0, reason
        
        # Required patterns check
        required_patterns = self.rules.get('required_patterns', [])
        if required_patterns:
            if not any(re.search(r'\\b' + re.escape(p.lower()) + r'\\b', text.lower()) for p in required_patterns):
                reason = f"ç¼ºå°‘å¿…éœ€çš„æ¨¡å¼: {required_patterns}"
                return False, 0.4, reason

        # Forbidden patterns check
        for pattern in self.rules.get('forbidden_patterns', []):
            if re.search(r'\\b' + re.escape(pattern.lower()) + r'\\b', text.lower()):
                reason = f"åŒ…å«ç¦æ­¢çš„æ¨¡å¼: '{pattern}'"
                return False, 0.0, reason
        
        return True, 0.6, "é€šè¿‡åŸºæœ¬æ£€æŸ¥" # Return a base score

    def check_quality(self, text: str, relevance_vector: Optional[List[float]] = None) -> Tuple[bool, float, str]:
        """æ£€æŸ¥å•ä¸ªæ–‡æœ¬çš„è´¨é‡"""
        is_valid, score, reason = self._check_basic_rules(text)
        if not is_valid:
            return False, score, reason

        # Smart semantic check
        if self.rules.get('enable_semantic_check', False) and relevance_vector:
            para_vector = get_embedding(text)
            if not para_vector:
                 return False, 0.0, "æ— æ³•ç”Ÿæˆ embedding"

            similarity = cosine_similarity(relevance_vector, para_vector)
            
            if similarity < self.rules['semantic_relevance_threshold']:
                reason = f"è¯­ä¹‰ç›¸å…³æ€§ä½ (å¾—åˆ†: {similarity:.2f}, é˜ˆå€¼: >{self.rules['semantic_relevance_threshold']})"
                logger.debug(f"[Quality Fail] {reason} | Content: '{text[:60]}...'")
                return False, similarity, reason
        
        quality_score = self._calculate_quality_score(text, text.split(), len(set(text.split())))
        return True, quality_score, "é€šè¿‡è´¨é‡æ£€æŸ¥"
    
    def _calculate_quality_score(self, text: str, words: List[str], unique_words: int) -> float:
        """è®¡ç®—è´¨é‡åˆ†æ•°"""
        score = 0.5  # åŸºç¡€åˆ†
        
        # é•¿åº¦å¥–åŠ±
        ideal_length = 500
        length_diff = abs(len(text) - ideal_length)
        length_score = max(0, 1 - length_diff / ideal_length)
        score += length_score * 0.2
        
        # è¯æ±‡ä¸°å¯Œåº¦å¥–åŠ±
        if len(words) > 0:
            richness = unique_words / len(words)
            score += richness * 0.3
        
        return min(1.0, score)

    def batch_check_quality(self, paragraphs: List[str], relevance_vector: Optional[List[float]] = None) -> List[Tuple[bool, float, str]]:
        """æ‰¹é‡æ£€æŸ¥æ®µè½è´¨é‡"""
        if not self.rules.get('enable_semantic_check', False) or relevance_vector is None:
            # å¦‚æœä¸è¿›è¡Œè¯­ä¹‰æ£€æŸ¥ï¼Œåˆ™åªè¿›è¡ŒåŸºæœ¬æ£€æŸ¥
            results = []
            for p in paragraphs:
                is_valid, score, reason = self._check_basic_rules(p)
                results.append((is_valid, score, reason))
            return results

        try:
            # æ‰¹é‡è·å– embedding
            para_vectors = get_embedding_batch(paragraphs)
            results = []
            semantic_threshold = self.rules.get('semantic_relevance_threshold', 0.4)

            for i, para_vector in enumerate(para_vectors):
                text = paragraphs[i]
                is_valid, _, reason = self._check_basic_rules(text)
                if not is_valid:
                    results.append((False, 0.0, reason))
                    continue

                if not para_vector: # Check if embedding failed for this item
                    results.append((False, 0.0, "Embedding generation failed"))
                    continue

                similarity = cosine_similarity(relevance_vector, para_vector)
                
                if similarity < semantic_threshold:
                    reason = f"è¯­ä¹‰ç›¸å…³æ€§ä½ (å¾—åˆ†: {similarity:.2f}, é˜ˆå€¼: >{semantic_threshold})"
                    logger.debug(f"[Quality Fail] {reason} | Content: '{text[:60]}...'")
                    results.append((False, similarity, reason))
                else:
                    quality_score = self._calculate_quality_score(text, text.split(), len(set(text.split())))
                    results.append((True, quality_score, "é€šè¿‡è´¨é‡æ£€æŸ¥"))
            
            return results
        except Exception as e:
            logger.error(f"æ‰¹é‡è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
            return [(False, 0.0, "æ£€æŸ¥å¼‚å¸¸") for _ in paragraphs]

    def should_send_for_review(self, score: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å°†å†…å®¹é€å®¡"""
        # åˆ†æ•°è¾ƒé«˜çš„å†…å®¹ï¼ˆå¯èƒ½æ˜¯è¾¹ç¼˜æƒ…å†µï¼‰é€å®¡
        return score > 0.2

UPDATE_QUEUE_FILE = project_root / "cache" / "update_queue.json"
PAGE_CONTENT_CACHE_DIR = project_root / "cache" / "page_content"

class GenericKnowledgeManager:
    """é€šç”¨çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, config_dir: str = "config/domains"):
        self.vector_store = VectorStoreFactory.get_vector_store()
        self.project_root = project_root
        self.config_dir = self.project_root / config_dir
        self.cache_dir = self.project_root / "cache" / "knowledge"
        self.review_dir = self.project_root / "data" / "needs_review"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.review_dir.mkdir(parents=True, exist_ok=True)
        PAGE_CONTENT_CACHE_DIR.mkdir(parents=True, exist_ok=True)
        
        # åŠ è½½é…ç½®
        self.domains = self._load_domains_config()
        self.parsers = self._load_parsers()
        
        self.browser = None
        self.playwright = None
    
    def _load_domains_config(self) -> Dict[str, DomainConfig]:
        """ä»ç›®å½•åŠ è½½æ‰€æœ‰é¢†åŸŸé…ç½®"""
        if not self.config_dir.exists() or not self.config_dir.is_dir():
            logger.warning(f"é…ç½®ç›®å½•ä¸å­˜åœ¨: {self.config_dir}")
            return self._create_example_config_structure()
        
        domains = {}
        for config_file in self.config_dir.glob("*.yaml"):
            domain_name = config_file.stem
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    domain_data = yaml.safe_load(f)

                if not domain_data:
                    logger.warning(f"é…ç½®æ–‡ä»¶ä¸ºç©º: {config_file}")
                    continue

                sources = []
                for source_data in domain_data.get('sources', []):
                    sources.append(SourceConfig(**source_data))
                
                domain = DomainConfig(
                    name=domain_name,
                    description=domain_data.get('description', ''),
                    collection_name=domain_data.get('collection_name', f"{domain_name}_docs"),
                    sources=sources,
                    parser_class=domain_data.get('parser_class'),
                    quality_rules=domain_data.get('quality_rules'),
                    test_queries=domain_data.get('test_queries'),
                    update_frequency_days=domain_data.get('update_frequency_days', 7),
                    enabled=domain_data.get('enabled', True)
                )
                domains[domain_name] = domain
            except Exception as e:
                logger.error(f"åŠ è½½é¢†åŸŸé…ç½®æ–‡ä»¶å¤±è´¥ {config_file}: {e}")
        
        return domains
    
    def _create_example_config_structure(self) -> Dict[str, DomainConfig]:
        """åˆ›å»ºç¤ºä¾‹é…ç½®ç›®å½•å’Œæ–‡ä»¶"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        example_file = self.config_dir / "immigration.yaml.example"
        
        example_config = {
            'description': 'Immigration and visa information',
            'collection_name': 'immigration_docs',
            'sources': [
                {
                    'name': 'IRCC_PNP',
                    'url': 'https://www.canada.ca/en/immigration-refugees-citizenship/services/immigrate-canada/provincial-nominees.html',
                    'type': 'website'
                }
            ],
            'quality_rules': {
                'min_length': 50,
                'required_patterns': ['immigration', 'visa', 'canada']
            },
            'test_queries': [
                {'query': 'How to apply for Express Entry?', 'relevant': True}
            ],
            'update_frequency_days': 7
        }
        
        with open(example_file, 'w', encoding='utf-8') as f:
            yaml.dump(example_config, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"å·²åˆ›å»ºç¤ºä¾‹é…ç½®ç›®å½•å’Œæ–‡ä»¶: {example_file}")
        return {}
    
    def _load_parsers(self) -> Dict[str, ContentParser]:
        """åŠ è½½è§£æå™¨"""
        parsers = {
            'generic': GenericHTMLParser()
        }
        
        # åŠ è½½è‡ªå®šä¹‰è§£æå™¨
        custom_parsers_dir = self.project_root / "parsers"
        if custom_parsers_dir.exists():
            for parser_file in custom_parsers_dir.glob("*.py"):
                if parser_file.name.startswith("_"):
                    continue
                
                try:
                    # åŠ¨æ€å¯¼å…¥è§£æå™¨
                    module_name = parser_file.stem
                    spec = importlib.util.spec_from_file_location(module_name, parser_file)
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    
                    # æŸ¥æ‰¾ContentParserå­ç±»
                    for name, obj in module.__dict__.items():
                        if (isinstance(obj, type) and 
                            issubclass(obj, ContentParser) and 
                            obj != ContentParser):
                            parsers[module_name] = obj()
                            logger.info(f"åŠ è½½è‡ªå®šä¹‰è§£æå™¨: {module_name}")
                            
                except Exception as e:
                    logger.error(f"åŠ è½½è§£æå™¨å¤±è´¥ {parser_file}: {e}")
        
        return parsers
    
    async def update_all_domains(self, force_update: bool = False):
        """æ›´æ–°æ‰€æœ‰å¯ç”¨çš„é¢†åŸŸ"""
        for domain_name, domain in self.domains.items():
            if domain.enabled:
                await self.update_domain(domain_name, force_update)
    
    async def update_domain(self, domain_name: str, force_update: bool = False):
        """æ›´æ–°ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†åº“"""
        if domain_name not in self.domains:
            logger.error(f"æœªæ‰¾åˆ°é¢†åŸŸé…ç½®: {domain_name}")
            return
        
        domain = self.domains[domain_name]
        if not domain.enabled:
            logger.info(f"é¢†åŸŸ {domain_name} å·²ç¦ç”¨ï¼Œè·³è¿‡æ›´æ–°")
            return
        
        logger.info(f"ğŸš€ æ›´æ–°é¢†åŸŸ: {domain_name}")
        
        if not HAS_PLAYWRIGHT:
            logger.error("âŒ Playwrightæœªå®‰è£…. è¯·è¿è¡Œ: pip install playwright && playwright install")
            return
        
        await self._initialize_browser()
        
        updated_count = 0
        for source_dict in domain.sources:
            # Convert dict to SourceConfig if needed
            if isinstance(source_dict, dict):
                source_config = SourceConfig(**source_dict)
            else:
                source_config = source_dict
                
            if source_config.type == 'website':
                success = await self._scrape_website(domain, source_config, force_update)
                if success:
                    updated_count += 1
            elif source_config.type == 'api':
                # TODO: å®ç°APIæ•°æ®æº
                logger.warning(f"APIæ•°æ®æºæš‚æœªå®ç°: {source_config.name}")
            elif source_config.type == 'file':
                # TODO: å®ç°æ–‡ä»¶æ•°æ®æº
                logger.warning(f"æ–‡ä»¶æ•°æ®æºæš‚æœªå®ç°: {source_config.name}")
        
        await self._close_browser()
        
        logger.info(f"âœ… é¢†åŸŸ {domain_name} æ›´æ–°å®Œæˆ! æ›´æ–°äº† {updated_count} ä¸ªæ•°æ®æº")
    
    async def _scrape_website(self, domain: DomainConfig, source: SourceConfig, 
                            force_update: bool) -> bool:
        """æŠ“å–ç½‘ç«™å†…å®¹"""
        if not force_update and not self._should_update_source(source, domain.update_frequency_days):
            logger.info(f"â­ï¸ {source.name}: æ— éœ€æ›´æ–°ï¼Œè·³è¿‡ã€‚")
            return False
        
        logger.info(f"ğŸŒ å¼€å§‹æŠ“å–: {source.name} ({source.url})")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·±å±‚çˆ¬å–
        deep_crawl = source.parser_config and source.parser_config.get('deep_crawl', False)
        max_depth = source.parser_config.get('max_depth', 3) if source.parser_config else 3
        max_pages = source.parser_config.get('max_pages', 50) if source.parser_config else 50
        
        if deep_crawl:
            logger.info(f"  - å¯ç”¨æ·±å±‚çˆ¬å–æ¨¡å¼ (æ·±åº¦: {max_depth}, æœ€å¤§é¡µé¢æ•°: {max_pages})")
            return await self._deep_crawl_website(domain, source, force_update, max_depth, max_pages)
        else:
            # åŸæœ‰çš„å•é¡µçˆ¬å–é€»è¾‘
            return await self._single_page_scrape(domain, source, force_update)
    
    async def _single_page_scrape(self, domain: DomainConfig, source: SourceConfig, 
                                 force_update: bool) -> bool:
        """æŠ“å–å•ä¸ªç½‘é¡µ"""
        try:
            context = await self.browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
            )
            page = await context.new_page()
            
            logger.debug(f"  - æ­£åœ¨å¯¼èˆªåˆ°é¡µé¢...")
            try:
                await page.goto(source.url, wait_until="networkidle", timeout=60000)
            except Exception as e: # Catch potential Playwright errors (like TimeoutError)
                logger.error(f"  - âŒ é¡µé¢å¯¼èˆªå¤±è´¥: {type(e).__name__}. ç½‘ç«™å¯èƒ½æ— æ³•è®¿é—®æˆ–å“åº”ç¼“æ…¢ã€‚")
                await context.close()
                return False

            await asyncio.sleep(2)
            
            html_content = await page.content()
            logger.debug(f"  - é¡µé¢HTMLå†…å®¹è·å–æˆåŠŸ (å¤§å°: {len(html_content)} bytes)ã€‚")
            await context.close()
            
            # --- Get Page Title for Semantic Context ---
            parser_name = domain.parser_class or 'generic'
            parser = self.parsers.get(parser_name, self.parsers['generic'])
            metadata = parser.extract_metadata(html_content, source.url)
            page_title = metadata.get('title')

            # --- Create Relevance Vector ---
            relevance_vector = None
            quality_rules = domain.quality_rules or {}
            if quality_rules.get('enable_semantic_check', False) and page_title:
                relevance_vector = get_embedding(page_title)
                logger.debug(f"  - è¯­ä¹‰ç›¸å…³æ€§æ£€æŸ¥å·²å¯ç”¨ã€‚åŸºå‡†æ ‡é¢˜: '{page_title}'")

            logger.debug("  - å¼€å§‹ä½¿ç”¨è§£æå™¨æå–å†…å®¹...")
            parser_config = source.parser_config or source.selectors or {}
            paragraphs = parser.parse(html_content, parser_config)
            logger.info(f"  - è§£æå™¨æå–äº† {len(paragraphs)} ä¸ªåˆå§‹æ®µè½ã€‚")
            
            if not paragraphs:
                logger.warning(f"   âš ï¸ è§£æå™¨æœªèƒ½ä»é¡µé¢æå–ä»»ä½•æœ‰æ•ˆæ®µè½ã€‚è¯·æ£€æŸ¥æºç½‘é¡µç»“æ„å’Œé…ç½®ä¸­çš„CSSé€‰æ‹©å™¨ã€‚")
                return False
            
            # --- Perform Quality Check with Semantic Context & Human Review ---
            logger.debug("  - å¼€å§‹è¿›è¡Œæ®µè½è´¨é‡æ£€æŸ¥...")
            quality_checker = QualityChecker(quality_rules)
            filtered_paragraphs = []
            paragraphs_for_review = []
            failure_reasons = Counter()

            # æ‰¹é‡æ£€æŸ¥
            quality_results = quality_checker.batch_check_quality(paragraphs, relevance_vector)

            # æ‰¹é‡å¤„ç†
            for paragraph, (passed, score, reason) in zip(paragraphs, quality_results):
                if passed:
                    filtered_paragraphs.append((paragraph, score))
                else:
                    # å¯¹äºæœªé€šè¿‡è´¨é‡æ£€æŸ¥çš„å†…å®¹ï¼Œæ ¹æ®åˆ†æ•°å†³å®šæ˜¯å¦é€å®¡
                    if score > 0.2:  # åˆ†æ•°è¾ƒé«˜çš„å†…å®¹é€å®¡
                        review_item = {
                            'url': source.url,
                            'content': paragraph,
                            'reason_for_filtering': reason,
                            'similarity_score': round(score, 4)
                        }
                        paragraphs_for_review.append(review_item)
                        
                        failure_type = reason.split('(')[0].strip()
                        failure_reasons[failure_type] += 1
                        logger.debug(f"    - [é€å¾€å®¡æŸ¥] (åŸå› : {reason}): '{paragraph[:100]}...'")
            
            if paragraphs_for_review:
                self._save_for_review(domain.name, source.name, paragraphs_for_review)

            logger.info(f"  - {len(filtered_paragraphs)}/{len(paragraphs)} ä¸ªæ®µè½é€šè¿‡è´¨é‡æ£€æŸ¥ã€‚")
            logger.info(f"  - {len(paragraphs_for_review)} ä¸ªæ®µè½å·²ä¿å­˜ä¾›äººå·¥å®¡æŸ¥ã€‚")

            if not filtered_paragraphs:
                logger.warning(f"   âš ï¸ æ²¡æœ‰æ®µè½ç›´æ¥é€šè¿‡è´¨é‡æ£€æŸ¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†æ­¤æºã€‚")
                top_3_reasons = failure_reasons.most_common(3)
                reasons_str = ", ".join([f"'{k}' ({v}æ¬¡)" for k, v in top_3_reasons])
                logger.warning(f"     - ä¸»è¦è¿‡æ»¤åŸå› : {reasons_str}")
                return False
            
            content_text = '\n'.join([p[0] for p in filtered_paragraphs])
            content_hash = hashlib.md5(content_text.encode()).hexdigest()
            
            if source.content_hash == content_hash and not force_update:
                logger.info(f"   âœ… å†…å®¹æ— å˜åŒ–ï¼Œæ— éœ€æ›´æ–°ã€‚")
                source.last_updated = datetime.now().isoformat()
                return True
            
            logger.debug(f"  - å†…å®¹æœ‰å˜åŒ–æˆ–è¢«å¼ºåˆ¶æ›´æ–°ï¼Œå‡†å¤‡å­˜å…¥å‘é‡æ•°æ®åº“...")
            await self._store_content(domain, source, filtered_paragraphs, metadata)
            
            source.content_hash = content_hash
            source.last_updated = datetime.now().isoformat()
            
            logger.info(f"   âœ… æˆåŠŸå¤„ç†å¹¶å­˜å‚¨äº† {len(filtered_paragraphs)} ä¸ªæ®µè½ã€‚")
            return True
            
        except Exception as e:
            logger.error(f"   âŒ æŠ“å–æˆ–å¤„ç†é¡µé¢æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {source.name} - {type(e).__name__}: {e}", exc_info=True)
            return False
    
    async def _deep_crawl_website(self, domain: DomainConfig, source: SourceConfig, 
                                force_update: bool, max_depth: int, max_pages: int) -> bool:
        """æ·±å±‚çˆ¬å–ç½‘ç«™ - æ™ºèƒ½é€‰æ‹©çˆ¬è™«ç±»å‹"""
        logger.info(f"  - å¼€å§‹æ·±å±‚çˆ¬å–: {source.url}")
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨Scrapyï¼ˆå¯¹äºé™æ€ç½‘ç«™ï¼‰
        use_scrapy = source.parser_config and source.parser_config.get('use_scrapy', False)
        
        # åˆå§‹åŒ–çˆ¬å–ç»“æœ
        results = []
        
        if use_scrapy:
            # ä½¿ç”¨Scrapyçˆ¬è™«ï¼ˆé€Ÿåº¦å¿«ï¼Œé€‚åˆé™æ€ç½‘ç«™ï¼‰
            logger.info(f"  - ä½¿ç”¨Scrapyçˆ¬è™«è¿›è¡Œé«˜é€Ÿçˆ¬å–")
            from scrapy_intelligent_crawler import ScrapyIntelligentCrawler
            
            try:
                # è·å–æ’é™¤æ¨¡å¼
                parser_config = source.parser_config or {}
                exclude_patterns = parser_config.get('exclude_link_patterns', [])
                
                # å‡†å¤‡å…³é”®è¯
                keywords = parser_config.get('keywords', ['immigration', 'visa', 'canada', 'permit'])
                
                # åˆ›å»ºScrapyçˆ¬è™«
                crawler = ScrapyIntelligentCrawler()
                
                # æ‰§è¡Œçˆ¬å–
                results, stats = crawler.crawl(
                    start_url=source.url,
                    max_depth=max_depth,
                    max_pages=max_pages,
                    exclude_patterns=exclude_patterns,
                    keywords=keywords,
                    importance_threshold=0.6
                )
                
                logger.info(f"  - Scrapyçˆ¬å–å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªé¡µé¢")
                
            except Exception as e:
                logger.error(f"  - Scrapyçˆ¬å–å¤±è´¥: {e}")
                logger.info(f"  - å›é€€åˆ°Playwrightçˆ¬è™«")
                use_scrapy = False
                results = []
                
        if not use_scrapy:
            # ä½¿ç”¨Playwrightçˆ¬è™«ï¼ˆåŠŸèƒ½å…¨é¢ï¼Œé€‚åˆåŠ¨æ€ç½‘ç«™ï¼‰
            logger.info(f"  - ä½¿ç”¨Playwrightçˆ¬è™«è¿›è¡Œæ™ºèƒ½çˆ¬å–")
            
            # å¯¼å…¥ç®€åŒ–ç‰ˆæ™ºèƒ½çˆ¬è™«
            from simple_intelligent_crawler import SimpleIntelligentCrawler
            
            # è·å–æ’é™¤æ¨¡å¼
            parser_config = source.parser_config or {}
            exclude_patterns = parser_config.get('exclude_link_patterns', [])
            
            # åˆ›å»ºçˆ¬è™«å®ä¾‹
            crawler = SimpleIntelligentCrawler(start_url=source.url)
            
            # æ‰§è¡Œçˆ¬å–
            results = await crawler.crawl(
                max_depth=max_depth,
                max_pages=max_pages,
                exclude_patterns=exclude_patterns
            )
            
            logger.info(f"  - çˆ¬å–å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªé¡µé¢")
            
            # ç”Ÿæˆçˆ¬å–æŠ¥å‘Š
            report_path = crawler.generate_report(domain.name, source.name)
            logger.info(f"  - çˆ¬å–æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        # å¤„ç†çˆ¬å–ç»“æœï¼ˆæ— è®ºä½¿ç”¨å“ªç§çˆ¬è™«ï¼‰
        try:
            # å‡†å¤‡è´¨é‡æ£€æŸ¥
            quality_rules = domain.quality_rules or {}
            quality_checker = QualityChecker(quality_rules)
            relevance_vector = None
            
            if quality_rules.get('enable_semantic_check'):
                relevance_text = f"{domain.description} {source.name}".strip()
                relevance_vector = get_embedding(relevance_text)
                logger.info(f"  - å·²ä¸ºé¢†åŸŸ '{domain.name}' ç”Ÿæˆç›¸å…³æ€§åŸºå‡†å‘é‡")
            
            # å¤„ç†çˆ¬å–ç»“æœ
            all_filtered_paragraphs = []
            
            for result in results:
                # ä½¿ç”¨è§£æå™¨å¤„ç†å†…å®¹
                parser_name = domain.parser_class or 'generic'
                parser = self.parsers.get(parser_name, self.parsers['generic'])
                
                # å°†çº¯æ–‡æœ¬å†…å®¹åˆ†æ®µå¹¶åŒ…è£…æˆHTMLæ®µè½
                # æŒ‰ç…§æ¢è¡Œç¬¦åˆ†å‰²æ–‡æœ¬ï¼Œåˆ›å»ºçœŸæ­£çš„HTMLæ®µè½
                text_paragraphs = result.content.split('\n\n')  # åŒæ¢è¡Œç¬¦åˆ†æ®µ
                html_paragraphs = []
                for para in text_paragraphs:
                    para = para.strip()
                    if para:  # å¿½ç•¥ç©ºæ®µè½
                        html_paragraphs.append(f"<p>{para}</p>")
                
                # ç”ŸæˆåŒ…å«çœŸæ­£æ®µè½çš„HTML
                html_content = f"""
                <html>
                <head><title>{result.title}</title></head>
                <body>
                {''.join(html_paragraphs)}
                </body>
                </html>
                """
                
                paragraphs = parser.parse(html_content, source.parser_config or {})
                if not paragraphs:
                    # å¦‚æœè§£æå™¨ä»ç„¶æ²¡æœ‰æå–åˆ°æ®µè½ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬æ®µè½
                    paragraphs = [para for para in text_paragraphs if para.strip() and len(para.strip()) >= 50]
                    logger.debug(f"  - è§£æå™¨æœªæå–åˆ°HTMLæ®µè½ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬æ®µè½: {len(paragraphs)} ä¸ª")
                
                if not paragraphs:
                    logger.warning(f"  - é¡µé¢ {result.url} æ²¡æœ‰æå–åˆ°ä»»ä½•æ®µè½")
                    continue
                
                logger.info(f"  - ä»é¡µé¢ {result.url} æå–åˆ° {len(paragraphs)} ä¸ªæ®µè½")
                
                metadata = {
                    'title': result.title,
                    'crawl_depth': result.depth,
                    'importance_score': result.importance_score,
                    'ai_summary': getattr(result, 'summary', '')  # å…¼å®¹ä¸¤ç§çˆ¬è™«çš„ç»“æœæ ¼å¼
                }
                
                # æ‰¹é‡è´¨é‡æ£€æŸ¥
                quality_results = quality_checker.batch_check_quality(paragraphs, relevance_vector)
                
                passed_count = 0
                failed_count = 0
                review_count = 0
                
                for paragraph, (passed, score, reason) in zip(paragraphs, quality_results):
                    if passed:
                        all_filtered_paragraphs.append((paragraph, score, result.url, metadata))
                        passed_count += 1
                    else:
                        failed_count += 1
                        if quality_checker.should_send_for_review(score):
                            review_count += 1
                            logger.debug(f"    - æ®µè½æœªé€šè¿‡ (åˆ†æ•°: {score:.3f}, åŸå› : {reason}): {paragraph[:50]}...")
                
                logger.info(f"  - è´¨é‡æ£€æŸ¥ç»“æœ: {passed_count} é€šè¿‡, {failed_count} å¤±è´¥, {review_count} å¾…å®¡æŸ¥")
            
            # å­˜å‚¨æ‰€æœ‰æ”¶é›†çš„å†…å®¹
            if all_filtered_paragraphs:
                logger.info(f"  - å…±æ”¶é›† {len(all_filtered_paragraphs)} ä¸ªé«˜è´¨é‡æ®µè½")
                
                # æ‰¹é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                collection_name = domain.collection_name
                self._ensure_collection(collection_name)
                
                points = []
                for paragraph, quality_score, url, metadata in all_filtered_paragraphs:
                    vector = get_embedding(paragraph)
                    point_id = str(uuid.uuid4())
                    
                    point_data = {
                        "id": point_id,
                        "vector": vector,
                        "payload": {
                            "content": paragraph,
                            "domain": domain.name,
                            "source": source.name,
                            "url": url,
                            "quality_score": quality_score,
                            "created_at": datetime.now().isoformat(),
                            **metadata
                        }
                    }
                    points.append(point_data)
                
                # æ‰¹é‡æ’å…¥
                self.vector_store.upsert(collection_name, points)
                
                # æ›´æ–°sourceçŠ¶æ€
                source.last_updated = datetime.now().isoformat()
                crawler_type = "scrapy_crawl" if use_scrapy else "deep_crawl"
                content_summary = f"{crawler_type}_{len(results)}_{len(all_filtered_paragraphs)}"
                source.content_hash = hashlib.md5(content_summary.encode()).hexdigest()
                
                logger.info(f"   âœ… æˆåŠŸå­˜å‚¨ {len(all_filtered_paragraphs)} ä¸ªæ®µè½åˆ°çŸ¥è¯†åº“")
                return True
            else:
                logger.warning(f"   âš ï¸ æ·±å±‚çˆ¬å–æœªèƒ½æ”¶é›†åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹")
                return False
                
        except Exception as e:
            logger.error(f"   âŒ æ·±å±‚çˆ¬å–å¤±è´¥: {e}", exc_info=True)
            return False
    
    def _save_for_review(self, domain_name: str, source_name: str, review_items: List[Dict]):
        """å°†éœ€è¦å®¡æŸ¥çš„æ®µè½ä¿å­˜ä¸ºYAMLå¿«ç…§æ–‡ä»¶"""
        # The filename will now be a .yaml file
        review_filename = f"{domain_name}_{source_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml"
        review_path = self.review_dir / review_filename

        # Add a 'status' field for the human reviewer
        for item in review_items:
            item['status'] = 'pending' # options: pending, approved, rejected

        snapshot_data = {
            'metadata': {
                'domain': domain_name,
                'source': source_name,
                'url': review_items[0]['url'] if review_items else 'N/A',
                'creation_date': datetime.now().isoformat(),
                'item_count': len(review_items)
            },
            'review_items': review_items
        }

        try:
            with open(review_path, 'w', encoding='utf-8') as f:
                # Use yaml.dump for a much more human-readable format
                yaml.dump(snapshot_data, f, allow_unicode=True, sort_keys=False, indent=2)
            logger.info(f"  - å·²å°† {len(review_items)} ä¸ªå¾…å®¡æ®µè½ä¿å­˜ä¸ºå¿«ç…§: {review_path}")
        except Exception as e:
            logger.error(f"   âŒ ä¿å­˜å¾…å®¡å¿«ç…§æ–‡ä»¶å¤±è´¥: {e}")
    
    async def _store_content(self, domain: DomainConfig, source: SourceConfig, 
                           paragraphs: List[Tuple[str, float]], metadata: Dict[str, Any]):
        """å­˜å‚¨å†…å®¹åˆ°å‘é‡æ•°æ®åº“"""
        collection_name = domain.collection_name
        
        # ç¡®ä¿é›†åˆå­˜åœ¨
        self._ensure_collection(collection_name)
        
        points = []
        
        for i, (paragraph, quality_score) in enumerate(paragraphs):
            vector = get_embedding(paragraph)
            
            # ä½¿ç”¨UUIDä½œä¸ºID
            point_id = str(uuid.uuid4())
            
            point_data = {
                "id": point_id,
                "vector": vector,
                "payload": {
                    "content": paragraph,
                    "domain": domain.name,
                    "source": source.name,
                    "url": source.url,
                    "quality_score": quality_score,
                    "created_at": datetime.now().isoformat(),
                    **metadata  # åŒ…å«è§£æå™¨æå–çš„å…ƒæ•°æ®
                }
            }
            points.append(point_data)
        
        # æ‰¹é‡æ’å…¥
        self.vector_store.upsert(collection_name, points)
    
    def _ensure_collection(self, collection_name: str):
        """ç¡®ä¿é›†åˆå­˜åœ¨"""
        try:
            self.vector_store.client.get_collection(collection_name)
        except:
            # åˆ›å»ºé›†åˆ
            self.vector_store.client.recreate_collection(
                collection_name=collection_name,
                vectors_config={
                    "size": 1536,  # OpenAI embedding size
                    "distance": "Cosine"
                }
            )
            logger.info(f"åˆ›å»ºé›†åˆ: {collection_name}")
    
    def _should_update_source(self, source: SourceConfig, update_frequency_days: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°æ•°æ®æº"""
        if not source.last_updated:
            return True
        
        try:
            last_update = datetime.fromisoformat(source.last_updated)
            days_since_update = (datetime.now() - last_update).days
            return days_since_update >= update_frequency_days
        except:
            return True
    
    async def _initialize_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        if self.browser:
            return
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=True, 
            args=['--no-sandbox', '--disable-dev-shm-usage']
        )
    
    async def _close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            await self.browser.close()
            self.browser = None
        if self.playwright:
            await self.playwright.stop()
            self.playwright = None
    
    def test_all_domains(self):
        """æµ‹è¯•æ‰€æœ‰å·²é…ç½®çš„é¢†åŸŸ"""
        logger.info("ğŸ”¬ å¼€å§‹æµ‹è¯•æ‰€æœ‰é¢†åŸŸ...")
        for domain_name, domain in self.domains.items():
            if domain.enabled and domain.test_queries:
                self.test_domain_queries(domain_name)
            else:
                logger.info(f"â­ï¸ è·³è¿‡é¢†åŸŸ {domain_name} (æœªå¯ç”¨æˆ–æ— æµ‹è¯•ç”¨ä¾‹)")

    def test_domain_queries(self, domain_name: str):
        """æµ‹è¯•é¢†åŸŸç‰¹å®šçš„æŸ¥è¯¢"""
        if domain_name not in self.domains:
            logger.error(f"æœªæ‰¾åˆ°é¢†åŸŸ: {domain_name}")
            return
        
        domain = self.domains[domain_name]
        collection_name = domain.collection_name
        
        test_queries_config = domain.test_queries
        if not test_queries_config:
            logger.warning(f"é¢†åŸŸ {domain_name} æ²¡æœ‰é…ç½®æµ‹è¯•ç”¨ä¾‹")
            return
            
        logger.info(f"ğŸ”¬ æµ‹è¯•é¢†åŸŸ {domain_name} çš„æŸ¥è¯¢...")
        
        correct_count = 0
        test_queries = [(item['query'], item['relevant']) for item in test_queries_config]
        
        for query, expected_relevant in test_queries:
            query_vector = get_embedding(query)
            
            try:
                results = self.vector_store.search(
                    collection_name=collection_name,
                    query_vector=query_vector,
                    limit=1
                )
                
                if results:
                    score = results[0].score
                    is_relevant = score > 0.5  # å¯é…ç½®çš„é˜ˆå€¼
                else:
                    score = 0.0
                    is_relevant = False
                
                is_correct = is_relevant == expected_relevant
                if is_correct:
                    correct_count += 1
                
                status = "âœ…" if is_correct else "âŒ"
                logger.info(f"{status} {query}: åˆ†æ•°={score:.3f}, ç›¸å…³={is_relevant}")
                
            except Exception as e:
                logger.error(f"âŒ {query}: æµ‹è¯•å¤±è´¥ - {e}")
        
        accuracy = correct_count / len(test_queries) if test_queries else 0
        logger.info(f"ğŸ“Š å‡†ç¡®ç‡: {accuracy:.1%} ({correct_count}/{len(test_queries)})")
    
    def get_statistics(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æ‰€æœ‰é¢†åŸŸçš„ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        
        for domain_name, domain in self.domains.items():
            try:
                collection_info = self.vector_store.client.get_collection(domain.collection_name)
                stats[domain_name] = {
                    'collection': domain.collection_name,
                    'description': domain.description,
                    'enabled': domain.enabled,
                    'sources_count': len(domain.sources),
                    'documents_count': collection_info.points_count,
                    'update_frequency_days': domain.update_frequency_days
                }
            except:
                stats[domain_name] = {
                    'collection': domain.collection_name,
                    'description': domain.description,
                    'enabled': domain.enabled,
                    'sources_count': len(domain.sources),
                    'documents_count': 0,
                    'update_frequency_days': domain.update_frequency_days,
                    'error': 'Collection not found'
                }
        
        return stats

    def process_reviews(self, dry_run: bool = True):
        """å¤„ç†äººå·¥å®¡æŸ¥è¿‡çš„å¿«ç…§æ–‡ä»¶"""
        logger.info(f"ğŸ” å¼€å§‹å¤„ç†å®¡æŸ¥æ–‡ä»¶... (æ¨¡å¼: {'æ¨¡æ‹Ÿ' if dry_run else 'ç”Ÿäº§'})")
        
        processed_dir = self.review_dir / "processed"
        processed_dir.mkdir(exist_ok=True)
        
        items_ingested = 0
        files_processed = 0

        for review_file in self.review_dir.glob("*.yaml"):
            logger.info(f"  - æ­£åœ¨å¤„ç†æ–‡ä»¶: {review_file.name}")
            try:
                with open(review_file, 'r', encoding='utf-8') as f:
                    snapshot_data = yaml.safe_load(f)

                approved_items = [
                    item for item in snapshot_data.get('review_items', [])
                    if item.get('status') == 'approved'
                ]

                if not approved_items:
                    logger.info("    - æœªå‘ç° 'approved' çŠ¶æ€çš„æ¡ç›®ï¼Œè·³è¿‡ã€‚")
                    continue

                logger.info(f"    - å‘ç° {len(approved_items)} ä¸ªå·²æ‰¹å‡†çš„æ¡ç›®ã€‚")
                domain_name = snapshot_data['metadata']['domain']
                domain = self.domains.get(domain_name)

                if not domain:
                    logger.error(f"    - âŒ é¢†åŸŸ '{domain_name}' çš„é…ç½®æœªæ‰¾åˆ°ï¼Œæ— æ³•å¤„ç†ã€‚")
                    continue

                # å°†æ‰¹å‡†çš„æ¡ç›®è½¬æ¢ä¸ºå­˜å‚¨æ ¼å¼
                paragraphs_to_store = [(item['content'], item.get('similarity_score', 0.5)) for item in approved_items]
                metadata = snapshot_data['metadata']

                if not dry_run:
                    # ä½¿ç”¨asyncio.runæ¥è°ƒç”¨å¼‚æ­¥çš„å­˜å‚¨å‡½æ•°
                    asyncio.run(self._store_content(domain, SourceConfig(name=metadata['source'], url=metadata['url'], type='manual'), paragraphs_to_store, metadata))
                
                items_ingested += len(approved_items)
                files_processed += 1

                if not dry_run:
                    # å°†å·²å¤„ç†çš„æ–‡ä»¶ç§»åŠ¨åˆ° processed æ–‡ä»¶å¤¹
                    shutil.move(str(review_file), str(processed_dir / review_file.name))
                    logger.info(f"    - âœ… æ–‡ä»¶å·²å¤„ç†å¹¶ç§»åŠ¨è‡³: {processed_dir.name}")

            except Exception as e:
                logger.error(f"  - âŒ å¤„ç†å®¡æŸ¥æ–‡ä»¶å¤±è´¥ {review_file.name}: {e}", exc_info=True)

        logger.info(f"âœ… å®¡æŸ¥å¤„ç†å®Œæˆã€‚å…±å¤„ç† {files_processed} ä¸ªæ–‡ä»¶ï¼Œ ingest äº† {items_ingested} ä¸ªæ¡ç›®ã€‚")

    async def _is_change_significant(self, url: str, old_content: str, new_content: str) -> Tuple[bool, str]:
        """ä½¿ç”¨LLMåˆ¤æ–­é¡µé¢å˜åŒ–æ˜¯å¦é‡å¤§"""
        from src.infrastructure.llm.factory import LLMFactory
        llm = LLMFactory.get_llm()

        # Simple hash check first to avoid LLM calls for identical content
        if hashlib.md5(old_content.encode()).hexdigest() == hashlib.md5(new_content.encode()).hexdigest():
            return False, "å†…å®¹å“ˆå¸Œå€¼æœªå˜"

        prompt = f"""
        Analyze the difference between the two versions of the webpage content below.
        A "significant" change involves updates to policy, numbers, dates, application requirements, or the addition/removal of key steps.
        Ignore minor changes like typo fixes, sentence restructuring, or formatting adjustments.

        OLD VERSION (summary):
        {old_content[:1500]}...

        NEW VERSION (summary):
        {new_content[:1500]}...

        Is the change significant? Respond in JSON format.
        Example:
        {{
          "is_significant": true,
          "reason": "The deadline was updated from 2024 to 2025."
        }}
        """
        try:
            response = llm.generate(prompt)
            result = json.loads(response.strip())
            is_significant = result.get("is_significant", False)
            reason = result.get("reason", "No specific reason provided.")
            logger.info(f"Semantic diff for {url}: Significant: {is_significant}, Reason: {reason}")
            return is_significant, reason
        except Exception as e:
            logger.error(f"Semantic diff failed for {url}: {e}")
            # Fallback to simple hash comparison on error
            return old_content != new_content, "LLM diff failed, fell back to hash comparison."

    def _get_cached_page_content(self, url: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„é¡µé¢å†…å®¹"""
        cache_file = PAGE_CONTENT_CACHE_DIR / f"{hashlib.md5(url.encode()).hexdigest()}.html"
        if cache_file.exists():
            return cache_file.read_text(encoding='utf-8')
        return None

    def _cache_page_content(self, url: str, content: str):
        """ç¼“å­˜é¡µé¢å†…å®¹"""
        cache_file = PAGE_CONTENT_CACHE_DIR / f"{hashlib.md5(url.encode()).hexdigest()}.html"
        cache_file.write_text(content, encoding='utf-8')

    async def update_incremental(self, force: bool = False):
        """
        æ‰§è¡Œå¢é‡æ›´æ–°ã€‚
        åªå¤„ç†åœ¨ `update_queue.json` ä¸­è¢«æ ‡è®°ä¸ºæœ‰æ›´æ–°çš„é¡µé¢ã€‚
        """
        logger.info("ğŸš€ Starting incremental update...")
        if not UPDATE_QUEUE_FILE.exists():
            logger.info("âœ… No update queue found. All content is up-to-date.")
            return

        with open(UPDATE_QUEUE_FILE, 'r') as f:
            update_queue = json.load(f)

        if not update_queue:
            logger.info("âœ… Update queue is empty. No work to do.")
            # Clean up empty queue file
            UPDATE_QUEUE_FILE.unlink()
            return
            
        await self._initialize_browser()
        
        updated_count = 0
        processed_urls = set()

        for item in update_queue:
            url = item['url']
            source_name = item['source_name']
            
            if url in processed_urls:
                continue

            logger.info(f"Processing updated URL from queue: {url}")
            
            # Find the corresponding domain and source config
            domain, source = None, None
            for d in self.domains.values():
                for s in d.sources:
                    # A bit loose, but should work for now.
                    if s.name == source_name:
                        domain, source = d, s
                        break
                if domain:
                    break
            
            if not domain or not source:
                logger.warning(f"Could not find matching domain/source for {source_name}. Skipping {url}.")
                continue

            try:
                page = await self.browser.new_page()
                await page.goto(url, wait_until='domcontentloaded', timeout=30000)
                new_content = await page.content()
                await page.close()
                
                old_content = self._get_cached_page_content(url)

                should_update = True
                if old_content and not force:
                    is_significant, reason = await self._is_change_significant(url, old_content, new_content)
                    if not is_significant:
                        should_update = False
                
                if should_update:
                    logger.info(f"âœ… Change is significant. Updating knowledge base for {url}.")
                    # Use existing parsing and storing logic
                    parser = self.parsers.get(domain.parser_class or 'generic', self.parsers['generic'])
                    paragraphs = parser.parse(new_content, source.parser_config or {})
                    metadata = parser.extract_metadata(new_content, url)
                    
                    quality_checker = QualityChecker(domain.quality_rules or {})
                    # Create relevance vector from domain desc/source name for better consistency
                    relevance_text = f"{domain.description} {source.name}".strip()
                    relevance_vector = get_embedding(relevance_text)
                    
                    quality_results = quality_checker.batch_check_quality(paragraphs, relevance_vector)
                    
                    filtered_paragraphs = [(p, score) for p, (passed, score, _) in zip(paragraphs, quality_results) if passed]

                    if filtered_paragraphs:
                        # We need a way to remove old docs for this URL before adding new ones
                        logger.info(f"UPSERTING: {len(filtered_paragraphs)} paragraphs for {url}")
                        await self._store_content(domain, source, filtered_paragraphs, metadata, specific_url=url)
                        updated_count += 1
                    
                    self._cache_page_content(url, new_content)
                else:
                    logger.info(f"Skipping update for {url}: change not significant.")

                processed_urls.add(url)

            except Exception as e:
                logger.error(f"Failed to process page {url}: {e}", exc_info=True)

        await self._close_browser()
        
        # Clean up the queue file after processing
        logger.info(f"Incremental update finished. Updated {updated_count} pages.")
        UPDATE_QUEUE_FILE.unlink()

    async def _store_content(self, domain: DomainConfig, source: SourceConfig, 
                           paragraphs: List[Tuple[str, float]], metadata: Dict[str, Any], specific_url: str = None):
        """å­˜å‚¨å†…å®¹åˆ°å‘é‡æ•°æ®åº“ï¼Œå¯é€‰æ‹©æ€§åœ°å…ˆåˆ é™¤ç‰¹å®šURLçš„æ—§å†…å®¹"""
        collection_name = domain.collection_name
        self._ensure_collection(collection_name)
        
        target_url = specific_url or source.url

        # **CRITICAL STEP**: Delete old entries for this specific URL to avoid duplicates
        self.vector_store.delete(
            collection_name=collection_name,
            filter_conditions={"must": [{"key": "payload.url", "match": {"value": target_url}}]}
        )
        logger.info(f"Deleted old vector entries for URL: {target_url}")

        points = []
        for i, (paragraph, quality_score) in enumerate(paragraphs):
            vector = get_embedding(paragraph)
            
            # ä½¿ç”¨UUIDä½œä¸ºID
            point_id = str(uuid.uuid4())
            
            point_data = {
                "id": point_id,
                "vector": vector,
                "payload": {
                    "content": paragraph,
                    "domain": domain.name,
                    "source": source.name,
                    "url": target_url, # Use the specific URL
                    "quality_score": quality_score,
                    "created_at": datetime.now().isoformat(),
                    **metadata
                }
            }
            points.append(point_data)
        self.vector_store.upsert(collection_name, points)

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é€šç”¨çŸ¥è¯†åº“ç®¡ç†ç³»ç»Ÿ")
    parser.add_argument("command", choices=['update', 'update-domain', 'test', 'stats', 'init', 'process-reviews', 'update-incremental'], help="è¦æ‰§è¡Œçš„å‘½ä»¤")
    parser.add_argument("--domain", help="æŒ‡å®šé¢†åŸŸåç§°")
    parser.add_argument("--config-dir", default="config/domains", help="é…ç½®ç›®å½•è·¯å¾„")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶æ›´æ–°")
    parser.add_argument("--log-level", default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"], help="è®¾ç½®æ—¥å¿—çº§åˆ«")
    parser.add_argument("--dry-run", action="store_true", help="æ¨¡æ‹Ÿè¿è¡Œï¼Œä¸æ‰§è¡Œå®é™…çš„æ•°æ®åº“å†™å…¥æˆ–æ–‡ä»¶ç§»åŠ¨")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—çº§åˆ«
    log_level = args.log_level.upper()
    logging.getLogger().setLevel(log_level)
    # æ§åˆ¶ç¬¬ä¸‰æ–¹åº“çš„æ—¥å¿—çº§åˆ«
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("httpcore").setLevel(logging.WARNING)
    logging.getLogger("openai").setLevel(logging.WARNING)
    logging.getLogger("websockets").setLevel(logging.WARNING)
    
    manager = GenericKnowledgeManager(config_dir=args.config_dir)
    
    if args.command == "init":
        # The init command already creates an example structure, so it's fine
        pass
            
    elif args.command == "update":
        await manager.update_all_domains(force_update=args.force)
        
    elif args.command == "update-domain":
        if not args.domain:
            print("âŒ è¯·ä½¿ç”¨ --domain æŒ‡å®šé¢†åŸŸåç§°")
            return
        await manager.update_domain(args.domain, force_update=args.force)
        
    elif args.command == "test":
        if not args.domain:
            print("âŒ è¯·ä½¿ç”¨ --domain æŒ‡å®šé¢†åŸŸåç§°")
            manager.test_all_domains()
        else:
            manager.test_domain_queries(args.domain)

    elif args.command == "stats":
        stats = manager.get_statistics()
        print("ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯:\n")
        for domain_name, domain_stats in stats.items():
            print(f"é¢†åŸŸ: {domain_name}")
            print(f"  æè¿°: {domain_stats['description']}")
            print(f"  çŠ¶æ€: {'å¯ç”¨' if domain_stats['enabled'] else 'ç¦ç”¨'}")
            print(f"  æ•°æ®æºæ•°: {domain_stats['sources_count']}")
            print(f"  æ–‡æ¡£æ•°: {domain_stats.get('documents_count', 0)}")
            print(f"  æ›´æ–°é¢‘ç‡: æ¯{domain_stats['update_frequency_days']}å¤©")
            if 'error' in domain_stats:
                print(f"  âš ï¸ é”™è¯¯: {domain_stats['error']}")
            print()

    elif args.command == "process-reviews":
        manager.process_reviews(dry_run=args.dry_run)

    elif args.command == "update-incremental":
        await manager.update_incremental(force_update=args.force)

if __name__ == "__main__":
    asyncio.run(main()) 