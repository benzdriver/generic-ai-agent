#!/usr/bin/env python3
"""
åŸºäºScrapyçš„æ™ºèƒ½çˆ¬è™« - é’ˆå¯¹é™æ€ç½‘ç«™çš„é«˜é€Ÿçˆ¬å–
"""

import sys
from pathlib import Path
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

import json
import logging
import re
from collections import Counter
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urlparse

import scrapy
import scrapy.signals
from scrapy.crawler import CrawlerProcess
from scrapy.http import Request, Response
from scrapy.linkextractors import LinkExtractor

logger = logging.getLogger(__name__)

@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœ"""
    url: str
    title: str
    content: str
    depth: int
    importance_score: float
    summary: str = ""
    
@dataclass
class CrawlStatistics:
    """çˆ¬å–ç»Ÿè®¡"""
    total_pages: int = 0
    total_links: int = 0
    pages_by_depth: Dict[int, int] = field(default_factory=dict)
    pages_by_importance: Counter = field(default_factory=Counter)
    skipped_low_importance: int = 0


class IntelligentSpider(scrapy.Spider):
    """æ™ºèƒ½Scrapyçˆ¬è™«"""
    
    name = 'intelligent_spider'
    custom_settings = {
        'ROBOTSTXT_OBEY': True,
        'CONCURRENT_REQUESTS': 32,  # é«˜å¹¶å‘
        'CONCURRENT_REQUESTS_PER_DOMAIN': 16,
        'DOWNLOAD_DELAY': 0.25,  # å°å»¶è¿Ÿ
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 0.25,
        'AUTOTHROTTLE_MAX_DELAY': 1,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 16,
        'COOKIES_ENABLED': False,
        'RETRY_TIMES': 2,
        'DOWNLOAD_TIMEOUT': 30,
        'LOG_LEVEL': 'INFO'
    }
    
    def __init__(self, start_url: str, max_depth: int = 3, max_pages: int = 100,
                 exclude_patterns: List[str] = None, keywords: List[str] = None,
                 importance_threshold: float = 0.6, crawler_instance=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        self.start_urls = [start_url]
        self.allowed_domains = [urlparse(start_url).netloc]
        self.max_depth = max_depth
        self.max_pages = max_pages
        self.exclude_patterns = exclude_patterns or []
        self.keywords = keywords or ['immigration', 'visa', 'permit', 'canada']
        self.importance_threshold = importance_threshold
        
        self.visited_urls: Set[str] = set()
        self.results: List[CrawlResult] = []
        self.stats = CrawlStatistics()
        
        # ä¿å­˜crawlerå®ä¾‹å¼•ç”¨
        self.crawler_instance = crawler_instance
        
        # é…ç½®é“¾æ¥æå–å™¨
        self.link_extractor = LinkExtractor(
            allow_domains=self.allowed_domains,
            deny=self.exclude_patterns,
            unique=True
        )
        
        # LLMæ¥å£ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self._llm = None
        
    @property
    def llm(self):
        """å»¶è¿Ÿåˆå§‹åŒ–LLM"""
        if self._llm is None:
            try:
                from src.infrastructure.llm.factory import LLMFactory
                self._llm = LLMFactory.get_llm()
            except:
                logger.warning("æ— æ³•åŠ è½½LLMï¼Œå°†è·³è¿‡AIæ‘˜è¦åŠŸèƒ½")
        return self._llm
        
    def start_requests(self):
        """ç”Ÿæˆåˆå§‹è¯·æ±‚"""
        for url in self.start_urls:
            yield Request(url, meta={'depth': 0})
            
    def parse(self, response: Response):
        """è§£æé¡µé¢"""
        url = response.url
        depth = response.meta['depth']
        
        # æ£€æŸ¥æ˜¯å¦å·²è®¿é—®æˆ–è¾¾åˆ°é™åˆ¶
        if url in self.visited_urls or len(self.results) >= self.max_pages:
            return
            
        self.visited_urls.add(url)
        self.stats.total_pages += 1
        self.stats.pages_by_depth[depth] = self.stats.pages_by_depth.get(depth, 0) + 1
        
        # æå–æ ‡é¢˜
        title = response.css('title::text').get() or 'No Title'
        
        # æå–ä¸»è¦å†…å®¹ - ä½¿ç”¨å¤šç§é€‰æ‹©å™¨
        content_selectors = [
            'main ::text',
            'article ::text', 
            '.content ::text',
            '#content ::text',
            'section ::text',
            'div.container ::text',
            'p::text',
            'li::text'
        ]
        
        content_parts = []
        for selector in content_selectors:
            texts = response.css(selector).getall()
            if texts:
                content_parts.extend(texts)
                
        # æ¸…ç†å’Œåˆå¹¶å†…å®¹
        content = ' '.join(content_parts)
        content = re.sub(r'\s+', ' ', content).strip()
        
        # è·³è¿‡å†…å®¹å¤ªå°‘çš„é¡µé¢
        if not content or len(content) < 100:
            logger.debug(f"é¡µé¢å†…å®¹å¤ªå°‘ï¼Œè·³è¿‡: {url}")
            return
            
        # å¿«é€Ÿè¯„ä¼°é¡µé¢é‡è¦æ€§
        importance_score = self._evaluate_importance_fast(title, content, url)
        
        if importance_score < 0.2:
            self.stats.skipped_low_importance += 1
            logger.debug(f"é¡µé¢é‡è¦æ€§å¤ªä½ ({importance_score:.2f}), è·³è¿‡: {url}")
            return
            
        # è·å–AIæ‘˜è¦ï¼ˆä»…å¯¹é«˜é‡è¦æ€§é¡µé¢ï¼‰
        summary = ""
        if importance_score >= self.importance_threshold and self.llm:
            try:
                summary = self._get_ai_summary(title, content[:1500])
            except Exception as e:
                logger.debug(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
                
        # ä¿å­˜ç»“æœ
        result = CrawlResult(
            url=url,
            title=title,
            content=content,
            depth=depth,
            importance_score=importance_score,
            summary=summary
        )
        self.results.append(result)
        
        # æ›´æ–°ç»Ÿè®¡
        if importance_score >= 0.8:
            self.stats.pages_by_importance['high'] += 1
        elif importance_score >= 0.5:
            self.stats.pages_by_importance['medium'] += 1
        else:
            self.stats.pages_by_importance['low'] += 1
            
        logger.info(f"å·²çˆ¬å– [{depth}] {url} | é‡è¦æ€§: {importance_score:.2f} | è¿›åº¦: {len(self.results)}/{self.max_pages}")
        
        # ç»§ç»­çˆ¬å–å­é“¾æ¥ï¼ˆå¦‚æœæ»¡è¶³æ¡ä»¶ï¼‰
        if importance_score >= self.importance_threshold and depth < self.max_depth:
            links = self.link_extractor.extract_links(response)
            self.stats.total_links += len(links)
            
            # ä¼˜å…ˆçº§æ’åº
            prioritized_links = self._prioritize_links(links)
            
            for link in prioritized_links[:30]:  # æ¯é¡µæœ€å¤š30ä¸ªé“¾æ¥
                if len(self.results) >= self.max_pages:
                    break
                    
                yield Request(
                    link.url,
                    meta={'depth': depth + 1},
                    priority=-depth  # Scrapyä¼˜å…ˆçº§ï¼ˆè´Ÿæ•°æ›´é«˜ï¼‰
                )
                
    def _evaluate_importance_fast(self, title: str, content: str, url: str) -> float:
        """å¿«é€Ÿè¯„ä¼°é¡µé¢é‡è¦æ€§ï¼ˆä¸ä½¿ç”¨LLMï¼‰"""
        score = 0.0
        
        # URLè¯„åˆ†
        url_lower = url.lower()
        url_keywords = ['immigrate', 'visa', 'permit', 'eligibility', 'apply', 
                       'requirements', 'process', 'guide', 'how-to']
        for keyword in url_keywords:
            if keyword in url_lower:
                score += 0.15
                
        # æ ‡é¢˜è¯„åˆ†
        title_lower = title.lower()
        for keyword in self.keywords:
            if keyword in title_lower:
                score += 0.2
                
        # å†…å®¹å…³é”®è¯å¯†åº¦
        content_lower = content.lower()
        keyword_count = sum(content_lower.count(kw) for kw in self.keywords)
        word_count = len(content_lower.split())
        if word_count > 0:
            keyword_density = keyword_count / word_count
            score += min(keyword_density * 10, 0.4)
            
        # å†…å®¹é•¿åº¦å¥–åŠ±
        if len(content) > 1000:
            score += 0.1
        if len(content) > 3000:
            score += 0.1
            
        return min(score, 1.0)
        
    def _get_ai_summary(self, title: str, content_preview: str) -> str:
        """è·å–AIæ‘˜è¦"""
        prompt = f"""
        Based on this immigration webpage, provide a 2-3 sentence summary:
        
        Title: {title}
        Content: {content_preview}
        
        Focus on key immigration information. Be concise and factual.
        """
        
        try:
            summary = self.llm.generate(prompt)
            return summary.strip()
        except:
            return ""
            
    def _prioritize_links(self, links) -> List:
        """å¯¹é“¾æ¥è¿›è¡Œä¼˜å…ˆçº§æ’åº"""
        scored_links = []
        
        priority_paths = ['apply', 'eligibility', 'requirements', 'guide', 'process']
        
        for link in links:
            score = 0
            url_lower = link.url.lower()
            
            # å…³é”®è¯åŒ¹é…
            for keyword in self.keywords:
                if keyword in url_lower:
                    score += 2
                    
            # ä¼˜å…ˆè·¯å¾„
            for path in priority_paths:
                if path in url_lower:
                    score += 3
                    
            # æ’é™¤å¤–éƒ¨é“¾æ¥
            if urlparse(link.url).netloc != self.allowed_domains[0]:
                score -= 10
                
            scored_links.append((score, link))
            
        scored_links.sort(key=lambda x: x[0], reverse=True)
        return [link for _, link in scored_links]


class ScrapyIntelligentCrawler:
    """Scrapyæ™ºèƒ½çˆ¬è™«ç®¡ç†å™¨"""
    
    def __init__(self):
        self.spider_results = []
        self.spider_stats = None
    
    def crawl(self, start_url: str, max_depth: int = 3, max_pages: int = 100,
              exclude_patterns: List[str] = None, keywords: List[str] = None,
              importance_threshold: float = 0.6) -> Tuple[List[CrawlResult], CrawlStatistics]:
        """æ‰§è¡Œçˆ¬å–"""
        
        # åˆ›å»ºè‡ªå®šä¹‰è®¾ç½®
        custom_settings = {
            'USER_AGENT': 'Mozilla/5.0 (compatible; IntelligentCrawler/1.0)',
        }
        
        # åˆ›å»ºçˆ¬è™«è¿›ç¨‹
        process = CrawlerProcess(custom_settings)
        
        # å¯åŠ¨çˆ¬å–
        logger.info(f"å¼€å§‹Scrapyçˆ¬å–: {start_url}")
        logger.info(f"é…ç½®: æ·±åº¦={max_depth}, æœ€å¤§é¡µé¢={max_pages}, é‡è¦æ€§é˜ˆå€¼={importance_threshold}")
        
        # åˆ›å»ºä¸€ä¸ªå›è°ƒæ¥æ•è·ç»“æœ
        def spider_closed(spider):
            self.spider_results = spider.results
            self.spider_stats = spider.stats
        
        # è·å–crawlerå¯¹è±¡å¹¶è¿æ¥ä¿¡å·
        crawler = process.create_crawler(IntelligentSpider)
        crawler.signals.connect(spider_closed, signal=scrapy.signals.spider_closed)
        
        # å¯åŠ¨çˆ¬è™«
        process.crawl(
            crawler,
            start_url=start_url,
            max_depth=max_depth,
            max_pages=max_pages,
            exclude_patterns=exclude_patterns or [],
            keywords=keywords or [],
            importance_threshold=importance_threshold,
            crawler_instance=self
        )
        
        process.start()  # é˜»å¡ç›´åˆ°å®Œæˆ
        
        # ç”ŸæˆæŠ¥å‘Š
        if self.spider_stats:
            self._print_statistics(self.spider_stats, len(self.spider_results))
            self._save_report(start_url, self.spider_results, self.spider_stats)
        
        return self.spider_results, self.spider_stats or CrawlStatistics()
        
    def _print_statistics(self, stats: CrawlStatistics, result_count: int):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n" + "="*50)
        logger.info("Scrapyçˆ¬å–ç»Ÿè®¡")
        logger.info("="*50)
        logger.info(f"æ€»é¡µé¢æ•°: {stats.total_pages}")
        logger.info(f"æˆåŠŸçˆ¬å–: {result_count}")
        logger.info(f"æ€»é“¾æ¥æ•°: {stats.total_links}")
        logger.info(f"æŒ‰æ·±åº¦åˆ†å¸ƒ: {dict(stats.pages_by_depth)}")
        logger.info(f"æŒ‰é‡è¦æ€§åˆ†å¸ƒ: {dict(stats.pages_by_importance)}")
        logger.info(f"è·³è¿‡çš„ä½é‡è¦æ€§é¡µé¢: {stats.skipped_low_importance}")
        
    def _save_report(self, start_url: str, results: List[CrawlResult], stats: CrawlStatistics):
        """ä¿å­˜çˆ¬å–æŠ¥å‘Š"""
        report_dir = Path("docs/crawl_reports")
        report_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now()
        timestamp_str = timestamp.strftime("%Y%m%d_%H%M%S")
        
        # ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š
        domain_name = urlparse(start_url).netloc.replace('.', '_')
        report_file_md = report_dir / f"scrapy_report_{domain_name}_{timestamp_str}.md"
        
        with open(report_file_md, 'w', encoding='utf-8') as f:
            # å†™å…¥æ ‡é¢˜å’Œå…ƒä¿¡æ¯
            f.write("# æ™ºèƒ½çˆ¬å–å®¡æŸ¥æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¥æœŸ:** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ç›®æ ‡ç½‘ç«™:** {start_url}\n")
            f.write(f"**çˆ¬å–æ–¹å¼:** Scrapyé«˜é€Ÿçˆ¬è™«\n")
            f.write(f"**æ€»è®¡é¡µé¢:** {len(results)}\n")
            f.write(f"**æˆåŠŸç‡:** {(len(results) / stats.total_pages * 100) if stats.total_pages > 0 else 0:.1f}%\n\n")
            
            # å†™å…¥ç»Ÿè®¡ä¿¡æ¯
            f.write("## çˆ¬å–ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»è®¿é—®é¡µé¢:** {stats.total_pages}\n")
            f.write(f"- **æ€»å‘ç°é“¾æ¥:** {stats.total_links}\n")
            f.write(f"- **è·³è¿‡ä½é‡è¦æ€§:** {stats.skipped_low_importance}\n")
            f.write(f"- **æ·±åº¦åˆ†å¸ƒ:** {dict(stats.pages_by_depth)}\n")
            f.write(f"- **é‡è¦æ€§åˆ†å¸ƒ:** é«˜({stats.pages_by_importance.get('high', 0)}) / ä¸­({stats.pages_by_importance.get('medium', 0)}) / ä½({stats.pages_by_importance.get('low', 0)})\n\n")
            
            # å†™å…¥é¡µé¢è¯¦æƒ…è¡¨æ ¼
            f.write("## çˆ¬å–é¡µé¢è¯¦æƒ…\n\n")
            f.write("| é‡è¦æ€§ | æ·±åº¦ | é“¾æ¥ | æ ‡é¢˜ | AI ç”Ÿæˆæ‘˜è¦ |\n")
            f.write("|:---:|:---:|:---|:---|:---|\n")
            
            # æŒ‰é‡è¦æ€§æ’åº
            sorted_results = sorted(results, key=lambda x: x.importance_score, reverse=True)
            
            for result in sorted_results:
                # é‡è¦æ€§æ˜Ÿçº§
                if result.importance_score >= 0.8:
                    importance = "â­â­â­"
                elif result.importance_score >= 0.5:
                    importance = "â­â­"
                else:
                    importance = "â­"
                
                # æˆªæ–­è¿‡é•¿çš„URLå’Œæ ‡é¢˜
                url_display = result.url
                if len(url_display) > 80:
                    url_display = url_display[:77] + "..."
                    
                title_display = result.title[:50] + "..." if len(result.title) > 50 else result.title
                
                # æ‘˜è¦å¤„ç†
                summary_display = result.summary if result.summary else "æ— æ‘˜è¦"
                if len(summary_display) > 150:
                    summary_display = summary_display[:147] + "..."
                
                # å†™å…¥è¡¨æ ¼è¡Œ
                f.write(f"| {importance} | {result.depth} | [{title_display}]({result.url}) | {title_display} | {summary_display} |\n")
            
            f.write("\n---\n")
            f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}*\n")
        
        logger.info(f"\nğŸ“„ MarkdownæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file_md}")
        
        # åŒæ—¶ä¿å­˜JSONæ ¼å¼ï¼ˆç”¨äºç¨‹åºå¤„ç†ï¼‰
        report_file_json = report_dir / f"scrapy_report_{domain_name}_{timestamp_str}.json"
        report_data = {
            "start_url": start_url,
            "timestamp": timestamp_str,
            "crawler_type": "scrapy",
            "statistics": {
                "total_pages": stats.total_pages,
                "successful_pages": len(results),
                "total_links": stats.total_links,
                "pages_by_depth": dict(stats.pages_by_depth),
                "pages_by_importance": dict(stats.pages_by_importance),
                "skipped_low_importance": stats.skipped_low_importance
            },
            "results": [
                {
                    "url": r.url,
                    "title": r.title,
                    "depth": r.depth,
                    "importance_score": r.importance_score,
                    "content_length": len(r.content),
                    "summary": r.summary
                }
                for r in results
            ]
        }
        
        with open(report_file_json, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"ğŸ“Š JSONæ•°æ®å·²ä¿å­˜è‡³: {report_file_json}")


def main():
    """æµ‹è¯•çˆ¬è™«"""
    crawler = ScrapyIntelligentCrawler()
    
    # æµ‹è¯•çˆ¬å–
    results, stats = crawler.crawl(
        start_url="https://www.canada.ca/en/immigration-refugees-citizenship/services/immigrate-canada/start-visa.html",
        max_depth=2,
        max_pages=20,
        exclude_patterns=["/fr/", "/news/", ".pdf", "/contact/"],
        keywords=['start-up', 'visa', 'entrepreneur', 'business', 'immigration'],
        importance_threshold=0.5
    )
    
    print(f"\nçˆ¬å–å®Œæˆ! å…±è·å– {len(results)} ä¸ªé¡µé¢")
    
    # æ˜¾ç¤ºæœ€é‡è¦çš„é¡µé¢
    print("\næœ€é‡è¦çš„5ä¸ªé¡µé¢:")
    for i, result in enumerate(sorted(results, key=lambda x: x.importance_score, reverse=True)[:5], 1):
        print(f"{i}. [{result.importance_score:.2f}] {result.title}")
        print(f"   URL: {result.url}")
        if result.summary:
            print(f"   æ‘˜è¦: {result.summary[:100]}...")


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    main() 