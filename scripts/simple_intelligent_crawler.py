#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ™ºèƒ½çˆ¬è™« - ä½¿ç”¨Playwrightå’ŒAIè¯„ä¼°è¿›è¡Œæ™ºèƒ½ç½‘ç«™çˆ¬å–
"""

import sys
from pathlib import Path
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

import asyncio
import logging
import json
from datetime import datetime
from typing import List, Dict
from urllib.parse import urlparse
from dataclasses import dataclass
import argparse

from playwright.async_api import async_playwright
from src.infrastructure.llm.factory import LLMFactory

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CrawlResult:
    """çˆ¬å–ç»“æœ"""
    url: str
    title: str
    content: str
    importance_score: float
    depth: int
    links_found: int
    summary: str = ""

class SimpleIntelligentCrawler:
    """ç®€åŒ–ç‰ˆæ™ºèƒ½çˆ¬è™«"""
    
    def __init__(self, start_url: str):
        self.start_url = start_url
        self.target_domain = urlparse(start_url).netloc
        self.visited_urls = set()
        self.results = []  # ä¿å­˜çˆ¬å–ç»“æœ
        self.stats = {}    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self.llm = None
        
    def _get_llm(self):
        """å»¶è¿ŸåŠ è½½LLM"""
        if self.llm is None:
            try:
                from src.infrastructure.llm.factory import LLMFactory
                self.llm = LLMFactory.get_llm()
            except:
                logger.warning("æ— æ³•åŠ è½½LLMï¼Œå°†è·³è¿‡AIåŠŸèƒ½")
        return self.llm
        
    async def crawl(self, max_depth: int = 3, max_pages: int = 100,
                   exclude_patterns: List[str] = None) -> List[CrawlResult]:
        """æ‰§è¡Œçˆ¬å–"""
        self.results = []
        url_queue = [(self.start_url, 0)]
        pages_crawled = 0
        exclude_patterns = exclude_patterns or []
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_pages': 0,
            'total_links': 0,
            'pages_by_depth': {},
            'pages_by_importance': {'high': 0, 'medium': 0, 'low': 0},
            'skipped_low_importance': 0
        }
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            
            try:
                while url_queue and pages_crawled < max_pages:
                    current_url, depth = url_queue.pop(0)
                    
                    if current_url in self.visited_urls or depth > max_depth:
                        continue
                        
                    # æ˜¾ç¤ºè¿›åº¦
                    queue_info = f"Queue: {len(url_queue)} | Visited: {len(self.visited_urls)}"
                    logger.info(f"Crawling: {current_url} (depth: {depth}) | {queue_info}")
                    
                    try:
                        page = await browser.new_page()
                        await page.goto(current_url, wait_until='domcontentloaded', timeout=30000)
                        
                        title = await page.title()
                        content = await page.content()
                        
                        # Extract text content
                        text_content = await page.evaluate("""
                            () => {
                                // Remove script and style elements
                                const scripts = document.querySelectorAll('script, style');
                                scripts.forEach(el => el.remove());
                                
                                // Get text content
                                return document.body.innerText || document.body.textContent || '';
                            }
                        """)
                        
                        # Extract links
                        links = await page.evaluate("""
                            () => {
                                const links = Array.from(document.querySelectorAll('a[href]'));
                                return links.map(link => ({
                                    href: link.href,
                                    text: link.textContent.trim()
                                }));
                            }
                        """)
                        
                        await page.close()
                        
                        # Evaluate importance
                        importance_score = await self._evaluate_importance(title, text_content[:3000])
                        
                        # Generate summary for important pages
                        summary = ""
                        if importance_score >= 0.6 and self._get_llm():
                            summary = await self._generate_summary(title, text_content[:3000])
                        
                        result = CrawlResult(
                            url=current_url,
                            title=title,
                            content=text_content,
                            importance_score=importance_score,
                            depth=depth,
                            links_found=len(links),
                            summary=summary
                        )
                        
                        self.results.append(result)
                        self.visited_urls.add(current_url)
                        pages_crawled += 1
                        
                        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                        self.stats['total_pages'] += 1
                        self.stats['total_links'] += len(links)
                        self.stats['pages_by_depth'][depth] = self.stats['pages_by_depth'].get(depth, 0) + 1
                        
                        if importance_score >= 0.8:
                            self.stats['pages_by_importance']['high'] += 1
                        elif importance_score >= 0.5:
                            self.stats['pages_by_importance']['medium'] += 1
                        else:
                            self.stats['pages_by_importance']['low'] += 1
                        
                        # æ™ºèƒ½å†³å®šæ˜¯å¦ç»§ç»­çˆ¬å–å­é“¾æ¥
                        if depth < max_depth and importance_score >= 0.6:
                            logger.info(f"  Page importance {importance_score:.2f} >= 0.6, will crawl child links")
                            
                            # æ ¹æ®é‡è¦æ€§åŠ¨æ€è°ƒæ•´è¦çˆ¬å–çš„é“¾æ¥æ•°é‡
                            max_links_to_add = int(30 * importance_score)
                            links_added = 0
                            
                            for link in links:
                                if links_added >= max_links_to_add:
                                    break
                                    
                                href = link['href']
                                if self._is_valid_url(href, exclude_patterns) and href not in self.visited_urls:
                                    # è¯„ä¼°é“¾æ¥æ–‡æœ¬çš„ç›¸å…³æ€§
                                    link_text = link['text'].lower()
                                    
                                    # é«˜ä¼˜å…ˆçº§å…³é”®è¯
                                    high_priority_keywords = ['visa', 'immigration', 'apply', 'eligibility', 
                                                            'requirement', 'process', 'permit', 'residency',
                                                            'start-up', 'entrepreneur']
                                    
                                    # ä¸­ç­‰ä¼˜å…ˆçº§å…³é”®è¯
                                    medium_priority_keywords = ['canada', 'program', 'application', 'form', 
                                                              'guide', 'document', 'fee', 'timeline']
                                    
                                    if any(kw in link_text for kw in high_priority_keywords):
                                        url_queue.insert(0, (href, depth + 1))  # æœ€é«˜ä¼˜å…ˆçº§
                                        links_added += 1
                                    elif any(kw in link_text for kw in medium_priority_keywords):
                                        insert_pos = min(len(url_queue) // 2, 10)
                                        url_queue.insert(insert_pos, (href, depth + 1))
                                        links_added += 1
                                    elif importance_score >= 0.8:
                                        url_queue.append((href, depth + 1))
                                        links_added += 1
                                        
                        elif importance_score < 0.6:
                            logger.info(f"  Page importance {importance_score:.2f} < 0.6, skipping child links")
                            self.stats['skipped_low_importance'] += 1
                        
                    except Exception as e:
                        logger.error(f"Error crawling {current_url}: {e}")
                        continue
                        
            finally:
                await browser.close()
                
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        logger.info("\n=== çˆ¬å–ç»Ÿè®¡ ===")
        logger.info(f"æ€»é¡µé¢æ•°: {len(self.results)}")
        logger.info(f"æ€»é“¾æ¥æ•°: {self.stats['total_links']}")
        logger.info(f"æŒ‰æ·±åº¦åˆ†å¸ƒ: {self.stats['pages_by_depth']}")
        logger.info(f"æŒ‰é‡è¦æ€§åˆ†å¸ƒ: {self.stats['pages_by_importance']}")
        logger.info(f"è·³è¿‡çš„ä½é‡è¦æ€§é¡µé¢: {self.stats['skipped_low_importance']}")
        
        return self.results
    
    async def _evaluate_importance(self, title: str, content: str) -> float:
        """è¯„ä¼°é¡µé¢é‡è¦æ€§"""
        # é¦–å…ˆå°è¯•ä½¿ç”¨AIè¯„ä¼°
        if self._get_llm():
            prompt = f"""
Rate the importance of this webpage for someone interested in Canadian immigration.
Title: {title}
Content preview: {content[:1000]}...

Rate from 0.0 to 1.0. Respond with ONLY the number.
"""
            try:
                response = self.llm.generate(prompt)
                return float(response.strip())
            except:
                pass
        
        # å¦‚æœAIä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºäºè§„åˆ™çš„è¯„ä¼°
        score = 0.0
        keywords = ['visa', 'immigration', 'permit', 'canada', 'apply', 'eligibility', 'requirement']
        
        title_lower = title.lower()
        content_lower = content.lower()
        
        # æ ‡é¢˜ä¸­çš„å…³é”®è¯
        for kw in keywords:
            if kw in title_lower:
                score += 0.15
                
        # å†…å®¹ä¸­çš„å…³é”®è¯å¯†åº¦
        keyword_count = sum(content_lower.count(kw) for kw in keywords)
        word_count = len(content_lower.split())
        if word_count > 0:
            keyword_density = keyword_count / word_count
            score += min(keyword_density * 10, 0.4)
            
        return min(score, 1.0)
            
    async def _generate_summary(self, title: str, content: str) -> str:
        """ç”Ÿæˆæ‘˜è¦"""
        prompt = f"""
Summarize this immigration webpage in 2-3 sentences.
Title: {title}
Content: {content[:2000]}...

Focus on key immigration information. Be concise and factual.
"""
        try:
            return self.llm.generate(prompt).strip()
        except:
            return ""
            
    def _is_valid_url(self, url: str, exclude_patterns: List[str]) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            if parsed.scheme not in ['http', 'https']:
                return False
            if self.target_domain not in parsed.netloc:
                return False
            
            # é»˜è®¤æ’é™¤æ¨¡å¼
            default_exclude = ['/fr/', '/fra/', '.pdf', '.doc', '.docx', '.xls', '.xlsx',
                             'mailto:', 'javascript:', 'tel:', '#', '/news/', '/videos/']
            
            # åˆå¹¶é»˜è®¤å’Œè‡ªå®šä¹‰æ’é™¤æ¨¡å¼
            all_exclude = default_exclude + exclude_patterns
            
            # æ£€æŸ¥URLæ˜¯å¦åŒ¹é…ä»»ä½•æ’é™¤æ¨¡å¼
            url_lower = url.lower()
            for pattern in all_exclude:
                if pattern in url_lower:
                    return False
                    
            return True
        except:
            return False
            
    def save_report(self, results: List[CrawlResult], output_file: Path):
        """ä¿å­˜æŠ¥å‘Š"""
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# æ™ºèƒ½çˆ¬å–æŠ¥å‘Š\n\n")
            f.write(f"**æ—¥æœŸ:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ä¸»é¢˜:** {self.topic}\n")
            f.write(f"**é¡µé¢æ•°:** {len(results)}\n\n")
            
            # Sort by importance
            sorted_results = sorted(results, key=lambda x: x.importance_score, reverse=True)
            
            f.write("## é¡µé¢åˆ—è¡¨\n\n")
            for i, result in enumerate(sorted_results, 1):
                f.write(f"### {i}. {result.title}\n")
                f.write(f"- **URL:** {result.url}\n")
                f.write(f"- **é‡è¦æ€§:** {result.importance_score:.2f}\n")
                f.write(f"- **æ·±åº¦:** {result.depth}\n")
                f.write(f"- **é“¾æ¥æ•°:** {result.links_found}\n")
                f.write(f"- **æ‘˜è¦:** {result.summary}\n\n")

    def generate_report(self, domain_name: str, source_name: str) -> Path:
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report_dir = Path("docs/crawl_reports")
        report_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now()
        timestamp_str = timestamp.strftime("%Y%m%d_%H%M%S")
        
        # ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š
        report_file_md = report_dir / f"playwright_report_{domain_name}_{source_name}_{timestamp_str}.md"
        
        with open(report_file_md, 'w', encoding='utf-8') as f:
            # å†™å…¥æ ‡é¢˜å’Œå…ƒä¿¡æ¯
            f.write("# æ™ºèƒ½çˆ¬å–å®¡æŸ¥æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¥æœŸ:** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**ç›®æ ‡ä¸»é¢˜:** {domain_name} - {source_name}\n")
            f.write(f"**ç›®æ ‡ç½‘ç«™:** {self.start_url}\n")
            f.write(f"**çˆ¬å–æ–¹å¼:** Playwrightæ™ºèƒ½çˆ¬è™«\n")
            f.write(f"**æ€»è®¡é¡µé¢:** {len(self.results)}\n\n")
            
            # å†™å…¥ç»Ÿè®¡ä¿¡æ¯
            f.write("## çˆ¬å–ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»è®¿é—®é¡µé¢:** {self.stats.get('total_pages', 0)}\n")
            f.write(f"- **æ€»å‘ç°é“¾æ¥:** {self.stats.get('total_links', 0)}\n")
            f.write(f"- **è·³è¿‡ä½é‡è¦æ€§:** {self.stats.get('skipped_low_importance', 0)}\n")
            f.write(f"- **æ·±åº¦åˆ†å¸ƒ:** {self.stats.get('pages_by_depth', {})}\n")
            
            pages_by_importance = self.stats.get('pages_by_importance', {})
            high = pages_by_importance.get('high', 0)
            medium = pages_by_importance.get('medium', 0) 
            low = pages_by_importance.get('low', 0)
            f.write(f"- **é‡è¦æ€§åˆ†å¸ƒ:** é«˜({high}) / ä¸­({medium}) / ä½({low})\n\n")
            
            # å†™å…¥é¡µé¢è¯¦æƒ…è¡¨æ ¼
            f.write("## çˆ¬å–é¡µé¢è¯¦æƒ…\n\n")
            f.write("| é‡è¦æ€§ | æ·±åº¦ | é“¾æ¥ | æ ‡é¢˜ | AI ç”Ÿæˆæ‘˜è¦ |\n")
            f.write("|:---:|:---:|:---|:---|:---|\n")
            
            # æŒ‰é‡è¦æ€§æ’åº
            sorted_results = sorted(self.results, key=lambda x: x.importance_score, reverse=True)
            
            for result in sorted_results:
                # é‡è¦æ€§æ˜Ÿçº§
                if result.importance_score >= 0.8:
                    importance = "â­â­â­"
                elif result.importance_score >= 0.5:
                    importance = "â­â­"
                else:
                    importance = "â­"
                
                # æˆªæ–­è¿‡é•¿çš„URLå’Œæ ‡é¢˜
                title_display = result.title[:50] + "..." if len(result.title) > 50 else result.title
                
                # æ‘˜è¦å¤„ç†
                summary_display = result.summary if result.summary else "æ— æ‘˜è¦"
                if len(summary_display) > 150:
                    summary_display = summary_display[:147] + "..."
                
                # å†™å…¥è¡¨æ ¼è¡Œ
                f.write(f"| {importance} | {result.depth} | [{title_display}]({result.url}) | {title_display} | {summary_display} |\n")
            
            f.write("\n---\n")
            f.write(f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}*\n")
        
        logger.info(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file_md}")
        
        # åŒæ—¶ä¿å­˜JSONæ ¼å¼ï¼ˆç”¨äºç¨‹åºå¤„ç†ï¼‰
        report_file_json = report_dir / f"playwright_report_{domain_name}_{source_name}_{timestamp_str}.json"
        report_data = {
            "domain": domain_name,
            "source": source_name,
            "start_url": self.start_url,
            "timestamp": timestamp_str,
            "crawler_type": "playwright",
            "statistics": self.stats,
            "results": [
                {
                    "url": r.url,
                    "title": r.title,
                    "depth": r.depth,
                    "importance_score": r.importance_score,
                    "content_length": len(r.content),
                    "summary": r.summary
                }
                for r in self.results
            ]
        }
        
        with open(report_file_json, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"ğŸ“Š JSONæ•°æ®å·²ä¿å­˜è‡³: {report_file_json}")
        
        return report_file_md

async def main():
    parser = argparse.ArgumentParser(description="ç®€åŒ–ç‰ˆæ™ºèƒ½çˆ¬è™«")
    parser.add_argument("--url", required=True, help="èµ·å§‹URL")
    parser.add_argument("--domain", default="immigration", help="é¢†åŸŸåç§°")
    parser.add_argument("--topic", default="Canadian immigration", help="ä¸»é¢˜")
    parser.add_argument("--max-pages", type=int, default=10, help="æœ€å¤§é¡µé¢æ•°")
    parser.add_argument("--max-depth", type=int, default=2, help="æœ€å¤§æ·±åº¦")
    
    args = parser.parse_args()
    
    crawler = SimpleIntelligentCrawler(
        start_url=args.url
    )
    
    print(f"ğŸš€ å¼€å§‹çˆ¬å–...")
    print(f"   URL: {args.url}")
    print(f"   æœ€å¤§é¡µé¢æ•°: {args.max_pages}")
    print(f"   æœ€å¤§æ·±åº¦: {args.max_depth}")
    
    results = await crawler.crawl(max_depth=args.max_depth, max_pages=args.max_pages)
    
    # Save report
    report_path = crawler.generate_report(args.domain, args.topic)
    
    print(f"âœ… çˆ¬å–å®Œæˆï¼æ‰¾åˆ° {len(results)} ä¸ªé¡µé¢")
    print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

if __name__ == "__main__":
    asyncio.run(main()) 