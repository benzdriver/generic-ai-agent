# ğŸ”„ AIé©±åŠ¨çš„å¢é‡æ›´æ–°ç³»ç»Ÿ

## ğŸ“‹ æ¦‚è¿°

åœ¨è‡ªä¸»çˆ¬è™«ç³»ç»ŸåŸºç¡€ä¸Šï¼Œå®ç°æ™ºèƒ½çš„å¢é‡æ›´æ–°æœºåˆ¶ï¼Œç¡®ä¿çŸ¥è¯†åº“å§‹ç»ˆä¿æŒæœ€æ–°ã€‚

## ğŸ—ï¸ å¢é‡æ›´æ–°æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å˜åŒ–æ£€æµ‹å™¨    â”‚ --> â”‚   ä¼˜å…ˆçº§è¯„ä¼°å™¨   â”‚ --> â”‚   å¢é‡çˆ¬å–å™¨     â”‚
â”‚ Change Detectorâ”‚     â”‚Priority Assessor â”‚     â”‚Incremental Crawlerâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                      â†“                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç‰ˆæœ¬å¯¹æ¯”å™¨    â”‚     â”‚   å†²çªè§£å†³å™¨    â”‚     â”‚   æ›´æ–°éªŒè¯å™¨     â”‚
â”‚Version Comparerâ”‚     â”‚Conflict Resolver â”‚     â”‚ Update Validator â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ æ ¸å¿ƒç»„ä»¶å®ç°

### 1. æ™ºèƒ½å˜åŒ–æ£€æµ‹å™¨ï¼ˆChange Detectorï¼‰

```python
class IntelligentChangeDetector:
    """ä½¿ç”¨å¤šç§ç­–ç•¥æ£€æµ‹ç½‘ç«™å˜åŒ–"""
    
    def __init__(self):
        self.llm = LLMFactory.get_llm()
        self.change_patterns = {}
        self.update_history = []
        
    async def detect_changes(self, monitored_urls: List[str]) -> Dict:
        """æ£€æµ‹éœ€è¦æ›´æ–°çš„å†…å®¹"""
        
        changes = {
            'critical_updates': [],    # å¿…é¡»ç«‹å³æ›´æ–°
            'regular_updates': [],     # å¸¸è§„æ›´æ–°
            'minor_changes': [],       # æ¬¡è¦å˜åŒ–
            'no_change': []           # æ— å˜åŒ–
        }
        
        for url in monitored_urls:
            # 1. å¿«é€Ÿæ£€æŸ¥ - HTTPå¤´ä¿¡æ¯
            quick_check = await self._quick_http_check(url)
            
            if quick_check['status'] == 'not_modified':
                changes['no_change'].append(url)
                continue
                
            # 2. å†…å®¹æŒ‡çº¹å¯¹æ¯”
            content_changed = await self._content_fingerprint_check(url)
            
            if not content_changed:
                changes['no_change'].append(url)
                continue
                
            # 3. AIè¯„ä¼°å˜åŒ–é‡è¦æ€§
            importance = await self._assess_change_importance(url)
            
            if importance['level'] == 'critical':
                changes['critical_updates'].append({
                    'url': url,
                    'reason': importance['reason'],
                    'detected_changes': importance['changes']
                })
            elif importance['level'] == 'regular':
                changes['regular_updates'].append(url)
            else:
                changes['minor_changes'].append(url)
                
        return changes
    
    async def _quick_http_check(self, url: str) -> Dict:
        """é€šè¿‡HTTPå¤´å¿«é€Ÿæ£€æŸ¥"""
        
        # è·å–å­˜å‚¨çš„ETagå’ŒLast-Modified
        stored_headers = await self.get_stored_headers(url)
        
        # å‘é€HEADè¯·æ±‚
        response = await self.http_client.head(url)
        
        # æ¯”è¾ƒETag
        if stored_headers.get('etag') == response.headers.get('ETag'):
            return {'status': 'not_modified'}
            
        # æ¯”è¾ƒLast-Modified
        if stored_headers.get('last_modified'):
            stored_time = parse_datetime(stored_headers['last_modified'])
            current_time = parse_datetime(response.headers.get('Last-Modified'))
            
            if current_time <= stored_time:
                return {'status': 'not_modified'}
                
        return {'status': 'modified'}
    
    async def _assess_change_importance(self, url: str) -> Dict:
        """ä½¿ç”¨AIè¯„ä¼°å˜åŒ–çš„é‡è¦æ€§"""
        
        # è·å–æ–°æ—§å†…å®¹
        old_content = await self.get_stored_content(url)
        new_content = await self.fetch_current_content(url)
        
        # è®¡ç®—å·®å¼‚
        diff = self._calculate_diff(old_content, new_content)
        
        # AIåˆ†æ
        importance_analysis = await self.llm.analyze(f"""
        Analyze the importance of these changes in immigration content:
        
        URL: {url}
        Changes detected: {diff['summary']}
        
        Old content preview: {old_content[:500]}
        New content preview: {new_content[:500]}
        
        Determine:
        1. Change importance level: critical/regular/minor
        2. What specifically changed (fees, requirements, deadlines, etc.)
        3. Whether this affects user applications
        
        Critical changes include:
        - Fee changes
        - Requirement changes
        - New deadlines
        - Policy updates
        - Program suspensions
        
        Return structured analysis.
        """)
        
        return importance_analysis
```

### 2. æ›´æ–°ä¼˜å…ˆçº§è¯„ä¼°å™¨ï¼ˆPriority Assessorï¼‰

```python
class UpdatePriorityAssessor:
    """è¯„ä¼°æ›´æ–°ä¼˜å…ˆçº§å¹¶åˆ¶å®šç­–ç•¥"""
    
    def __init__(self):
        self.program_importance = {
            'express_entry': 10,
            'pnp': 9,
            'start_up_visa': 8,
            'family_sponsorship': 9,
            'work_permit': 8,
            'study_permit': 7
        }
        
    async def prioritize_updates(self, detected_changes: Dict) -> List[Dict]:
        """åˆ¶å®šæ›´æ–°ä¼˜å…ˆçº§"""
        
        prioritized_updates = []
        
        # 1. å¤„ç†å…³é”®æ›´æ–°
        for change in detected_changes['critical_updates']:
            priority_score = await self._calculate_priority_score(change)
            
            prioritized_updates.append({
                'url': change['url'],
                'priority': priority_score,
                'update_strategy': 'immediate_full',
                'reason': change['reason'],
                'affected_programs': await self._identify_affected_programs(change)
            })
        
        # 2. æ™ºèƒ½æ‰¹å¤„ç†å¸¸è§„æ›´æ–°
        batched_updates = await self._batch_similar_updates(
            detected_changes['regular_updates']
        )
        
        for batch in batched_updates:
            prioritized_updates.append({
                'urls': batch['urls'],
                'priority': batch['priority'],
                'update_strategy': 'batch_incremental',
                'estimated_time': batch['estimated_time']
            })
        
        # 3. å»¶è¿Ÿå¤„ç†æ¬¡è¦æ›´æ–°
        if detected_changes['minor_changes']:
            prioritized_updates.append({
                'urls': detected_changes['minor_changes'],
                'priority': 1,
                'update_strategy': 'delayed_batch',
                'schedule': 'next_maintenance_window'
            })
        
        return sorted(prioritized_updates, key=lambda x: x['priority'], reverse=True)
    
    async def _calculate_priority_score(self, change: Dict) -> float:
        """è®¡ç®—æ›´æ–°ä¼˜å…ˆçº§åˆ†æ•°"""
        
        score = 0.0
        
        # 1. åŸºäºå—å½±å“çš„ç§»æ°‘é¡¹ç›®
        for program in change.get('affected_programs', []):
            score += self.program_importance.get(program, 5)
        
        # 2. åŸºäºå˜åŒ–ç±»å‹
        change_type_scores = {
            'fee_change': 10,
            'requirement_change': 9,
            'deadline_change': 10,
            'process_change': 7,
            'form_update': 6
        }
        
        for change_type in change.get('detected_changes', []):
            score += change_type_scores.get(change_type, 3)
        
        # 3. åŸºäºç”¨æˆ·æŸ¥è¯¢é¢‘ç‡
        query_frequency = await self._get_query_frequency(change['url'])
        score += min(query_frequency / 100, 5)  # æœ€å¤šåŠ 5åˆ†
        
        # 4. åŸºäºä¸Šæ¬¡æ›´æ–°æ—¶é—´
        days_since_update = await self._get_days_since_last_update(change['url'])
        if days_since_update > 30:
            score += 3
        
        return min(score, 100)  # æœ€é«˜100åˆ†
```

### 3. å¢é‡çˆ¬å–å™¨ï¼ˆIncremental Crawlerï¼‰

```python
class IncrementalCrawler:
    """æ‰§è¡Œå¢é‡æ›´æ–°çš„æ™ºèƒ½çˆ¬è™«"""
    
    async def perform_incremental_update(self, update_plan: List[Dict]) -> Dict:
        """æ‰§è¡Œå¢é‡æ›´æ–°"""
        
        update_results = {
            'successful_updates': [],
            'failed_updates': [],
            'new_discoveries': [],
            'removed_content': []
        }
        
        for update_task in update_plan:
            if update_task['update_strategy'] == 'immediate_full':
                # ç«‹å³å®Œæ•´æ›´æ–°
                result = await self._full_page_update(update_task['url'])
                
            elif update_task['update_strategy'] == 'batch_incremental':
                # æ‰¹é‡å¢é‡æ›´æ–°
                result = await self._batch_incremental_update(update_task['urls'])
                
            elif update_task['update_strategy'] == 'delayed_batch':
                # å»¶è¿Ÿæ‰¹é‡æ›´æ–°
                result = await self._schedule_delayed_update(update_task['urls'])
            
            # å¤„ç†æ›´æ–°ç»“æœ
            await self._process_update_result(result, update_results)
        
        return update_results
    
    async def _full_page_update(self, url: str) -> Dict:
        """å®Œæ•´é¡µé¢æ›´æ–°"""
        
        # 1. çˆ¬å–æ–°å†…å®¹
        new_content = await self._crawl_with_retries(url)
        
        # 2. æ·±åº¦å†…å®¹æå–
        extracted = await SmartContentExtractor().extract_with_context(
            new_content['page'], 
            url
        )
        
        # 3. æ£€æµ‹æ–°å¢é“¾æ¥
        new_links = await self._detect_new_links(url, extracted['links'])
        
        # 4. ç‰ˆæœ¬å¯¹æ¯”
        version_diff = await self._compare_versions(url, extracted)
        
        # 5. æ›´æ–°çŸ¥è¯†åº“
        update_result = await self._update_knowledge_base(
            url,
            extracted,
            version_diff
        )
        
        # 6. å¦‚æœå‘ç°æ–°çš„é‡è¦é“¾æ¥ï¼Œæ·»åŠ åˆ°çˆ¬å–é˜Ÿåˆ—
        if new_links['important_links']:
            await self._queue_new_discoveries(new_links['important_links'])
        
        return {
            'url': url,
            'status': 'success',
            'changes': version_diff,
            'new_links': new_links
        }
    
    async def _smart_partial_update(self, url: str, changed_sections: List[str]) -> Dict:
        """æ™ºèƒ½éƒ¨åˆ†æ›´æ–° - åªæ›´æ–°å˜åŒ–çš„éƒ¨åˆ†"""
        
        # 1. å®šä½å˜åŒ–çš„åŒºåŸŸ
        page = await self._get_page(url)
        
        updates = {}
        for section_id in changed_sections:
            # 2. åªæå–å˜åŒ–çš„éƒ¨åˆ†
            section_content = await page.query_selector(f'#{section_id}')
            if section_content:
                updates[section_id] = await section_content.inner_text()
        
        # 3. åˆå¹¶åˆ°ç°æœ‰å†…å®¹
        merged_content = await self._merge_partial_updates(url, updates)
        
        return merged_content
```

### 4. ç‰ˆæœ¬æ§åˆ¶å’Œå¯¹æ¯”å™¨ï¼ˆVersion Comparerï¼‰

```python
class VersionComparer:
    """æ™ºèƒ½ç‰ˆæœ¬å¯¹æ¯”å’Œå˜åŒ–è¿½è¸ª"""
    
    def __init__(self):
        self.version_store = {}
        
    async def track_content_evolution(self, url: str, new_content: Dict) -> Dict:
        """è¿½è¸ªå†…å®¹æ¼”å˜"""
        
        # 1. è·å–å†å²ç‰ˆæœ¬
        history = await self._get_version_history(url)
        
        # 2. ç”Ÿæˆå†…å®¹å¿«ç…§
        snapshot = {
            'timestamp': datetime.now(),
            'content_hash': self._generate_content_hash(new_content),
            'structured_data': new_content['structured_data'],
            'metadata': {
                'fees': self._extract_fees(new_content),
                'requirements': self._extract_requirements(new_content),
                'timelines': self._extract_timelines(new_content),
                'forms': self._extract_forms(new_content)
            }
        }
        
        # 3. æ™ºèƒ½å¯¹æ¯”
        if history:
            latest_version = history[-1]
            changes = await self._intelligent_diff(latest_version, snapshot)
            
            # 4. ç”Ÿæˆå˜åŒ–æ‘˜è¦
            change_summary = await self.llm.summarize_changes(f"""
            Summarize these immigration content changes:
            
            Old version: {latest_version['metadata']}
            New version: {snapshot['metadata']}
            
            Focus on:
            1. Fee changes (amounts and types)
            2. Requirement changes (added/removed/modified)
            3. Timeline changes
            4. New forms or documents
            
            Generate a clear, concise summary for users.
            """)
            
            snapshot['change_summary'] = change_summary
            snapshot['change_details'] = changes
        
        # 5. ä¿å­˜æ–°ç‰ˆæœ¬
        await self._save_version(url, snapshot)
        
        return snapshot
    
    async def _intelligent_diff(self, old_version: Dict, new_version: Dict) -> Dict:
        """æ™ºèƒ½å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬"""
        
        changes = {
            'added': {},
            'removed': {},
            'modified': {},
            'impact_level': 'low'
        }
        
        # 1. å¯¹æ¯”è´¹ç”¨
        fee_changes = self._compare_fees(
            old_version['metadata']['fees'],
            new_version['metadata']['fees']
        )
        if fee_changes:
            changes['modified']['fees'] = fee_changes
            changes['impact_level'] = 'high'
        
        # 2. å¯¹æ¯”è¦æ±‚
        req_changes = self._compare_requirements(
            old_version['metadata']['requirements'],
            new_version['metadata']['requirements']
        )
        if req_changes:
            changes['modified']['requirements'] = req_changes
            changes['impact_level'] = 'high'
        
        # 3. å¯¹æ¯”æ—¶é—´çº¿
        timeline_changes = self._compare_timelines(
            old_version['metadata']['timelines'],
            new_version['metadata']['timelines']
        )
        if timeline_changes:
            changes['modified']['timelines'] = timeline_changes
            if changes['impact_level'] != 'high':
                changes['impact_level'] = 'medium'
        
        return changes
```

### 5. å†²çªè§£å†³å™¨ï¼ˆConflict Resolverï¼‰

```python
class ConflictResolver:
    """è§£å†³æ›´æ–°è¿‡ç¨‹ä¸­çš„å†²çª"""
    
    async def resolve_update_conflicts(self, conflicts: List[Dict]) -> Dict:
        """æ™ºèƒ½è§£å†³æ›´æ–°å†²çª"""
        
        resolutions = []
        
        for conflict in conflicts:
            if conflict['type'] == 'contradictory_information':
                # ä¿¡æ¯çŸ›ç›¾
                resolution = await self._resolve_contradiction(conflict)
                
            elif conflict['type'] == 'source_reliability':
                # æ¥æºå¯é æ€§å†²çª
                resolution = await self._resolve_source_conflict(conflict)
                
            elif conflict['type'] == 'temporal_conflict':
                # æ—¶é—´å†²çªï¼ˆæ–°æ—§ä¿¡æ¯ï¼‰
                resolution = await self._resolve_temporal_conflict(conflict)
                
            resolutions.append(resolution)
        
        return {
            'resolutions': resolutions,
            'confidence': self._calculate_resolution_confidence(resolutions)
        }
    
    async def _resolve_contradiction(self, conflict: Dict) -> Dict:
        """è§£å†³ä¿¡æ¯çŸ›ç›¾"""
        
        # ä½¿ç”¨LLMåˆ†æçŸ›ç›¾
        analysis = await self.llm.analyze(f"""
        Resolve this contradiction in immigration information:
        
        Source 1: {conflict['source1']['url']}
        Content: {conflict['source1']['content']}
        
        Source 2: {conflict['source2']['url']}
        Content: {conflict['source2']['content']}
        
        Consider:
        1. Official source priority
        2. Publication dates
        3. Specific vs general information
        4. Context and conditions
        
        Determine the correct information and explain.
        """)
        
        return {
            'conflict_id': conflict['id'],
            'resolution': analysis['correct_information'],
            'reasoning': analysis['explanation'],
            'confidence': analysis['confidence']
        }
```

### 6. æ›´æ–°è°ƒåº¦å™¨ï¼ˆUpdate Schedulerï¼‰

```python
class IntelligentUpdateScheduler:
    """æ™ºèƒ½è°ƒåº¦æ›´æ–°ä»»åŠ¡"""
    
    def __init__(self):
        self.update_patterns = {}
        self.resource_monitor = ResourceMonitor()
        
    async def create_update_schedule(self, monitored_content: Dict) -> Dict:
        """åˆ›å»ºæ™ºèƒ½æ›´æ–°è°ƒåº¦"""
        
        schedule = {
            'real_time': [],      # å®æ—¶ç›‘æ§
            'hourly': [],         # æ¯å°æ—¶æ£€æŸ¥
            'daily': [],          # æ¯æ—¥æ£€æŸ¥
            'weekly': [],         # æ¯å‘¨æ£€æŸ¥
            'monthly': []         # æ¯æœˆæ£€æŸ¥
        }
        
        for url, metadata in monitored_content.items():
            # 1. åŸºäºå†…å®¹ç±»å‹ç¡®å®šæ›´æ–°é¢‘ç‡
            content_type = metadata.get('content_type')
            
            if content_type in ['processing_times', 'application_status']:
                # å¤„ç†æ—¶é—´å’ŒçŠ¶æ€éœ€è¦é¢‘ç¹æ›´æ–°
                schedule['hourly'].append(url)
                
            elif content_type in ['fees', 'requirements']:
                # è´¹ç”¨å’Œè¦æ±‚æ¯æ—¥æ£€æŸ¥
                schedule['daily'].append(url)
                
            elif content_type in ['program_overview', 'eligibility']:
                # é¡¹ç›®æ¦‚è¿°æ¯å‘¨æ£€æŸ¥
                schedule['weekly'].append(url)
                
            else:
                # å…¶ä»–å†…å®¹æ¯æœˆæ£€æŸ¥
                schedule['monthly'].append(url)
            
            # 2. åŸºäºå†å²æ›´æ–°æ¨¡å¼è°ƒæ•´
            update_pattern = await self._analyze_update_pattern(url)
            
            if update_pattern['frequency'] == 'high':
                # å‡çº§æ›´æ–°é¢‘ç‡
                self._promote_update_frequency(schedule, url)
            
            # 3. åŸºäºç”¨æˆ·æŸ¥è¯¢çƒ­åº¦è°ƒæ•´
            query_heat = await self._get_query_heat(url)
            
            if query_heat > 0.8:  # é«˜çƒ­åº¦å†…å®¹
                self._promote_update_frequency(schedule, url)
        
        # 4. ä¼˜åŒ–è°ƒåº¦é¿å…èµ„æºå†²çª
        optimized_schedule = await self._optimize_schedule(schedule)
        
        return optimized_schedule
    
    async def _analyze_update_pattern(self, url: str) -> Dict:
        """åˆ†æå†å²æ›´æ–°æ¨¡å¼"""
        
        history = await self._get_update_history(url)
        
        if not history:
            return {'frequency': 'unknown', 'pattern': None}
        
        # è®¡ç®—æ›´æ–°é—´éš”
        intervals = []
        for i in range(1, len(history)):
            interval = (history[i]['timestamp'] - history[i-1]['timestamp']).days
            intervals.append(interval)
        
        # åˆ†ææ¨¡å¼
        avg_interval = sum(intervals) / len(intervals) if intervals else 30
        
        if avg_interval < 7:
            return {'frequency': 'high', 'avg_days': avg_interval}
        elif avg_interval < 30:
            return {'frequency': 'medium', 'avg_days': avg_interval}
        else:
            return {'frequency': 'low', 'avg_days': avg_interval}
```

### 7. å®æ—¶ç›‘æ§ç³»ç»Ÿ

```python
class RealTimeMonitor:
    """å®æ—¶ç›‘æ§å…³é”®é¡µé¢å˜åŒ–"""
    
    def __init__(self):
        self.websocket_connections = {}
        self.rss_feeds = {}
        self.api_endpoints = {}
        
    async def setup_real_time_monitoring(self, critical_urls: List[str]):
        """è®¾ç½®å®æ—¶ç›‘æ§"""
        
        for url in critical_urls:
            # 1. æ£€æŸ¥æ˜¯å¦æœ‰RSSè®¢é˜…
            rss_feed = await self._discover_rss_feed(url)
            if rss_feed:
                self.rss_feeds[url] = await self._subscribe_rss(rss_feed)
            
            # 2. æ£€æŸ¥æ˜¯å¦æœ‰API
            api_endpoint = await self._discover_api(url)
            if api_endpoint:
                self.api_endpoints[url] = api_endpoint
            
            # 3. è®¾ç½®é¡µé¢å˜åŒ–ç›‘æ§
            await self._setup_page_monitor(url)
    
    async def _setup_page_monitor(self, url: str):
        """è®¾ç½®é¡µé¢å˜åŒ–ç›‘æ§"""
        
        # ä½¿ç”¨è½®è¯¢+æ™ºèƒ½æ£€æµ‹
        monitor_config = {
            'url': url,
            'check_interval': 300,  # 5åˆ†é’Ÿ
            'change_threshold': 0.05,  # 5%å˜åŒ–è§¦å‘æ›´æ–°
            'focus_selectors': [  # é‡ç‚¹ç›‘æ§çš„å…ƒç´ 
                '.alert',  # è­¦å‘Šä¿¡æ¯
                '.update-notice',  # æ›´æ–°é€šçŸ¥
                '.fees-table',  # è´¹ç”¨è¡¨
                '.processing-times',  # å¤„ç†æ—¶é—´
                '.requirements'  # è¦æ±‚åˆ—è¡¨
            ]
        }
        
        asyncio.create_task(self._monitor_loop(monitor_config))
```

## ğŸ”„ å®Œæ•´å¢é‡æ›´æ–°æµç¨‹

```python
class IncrementalUpdateOrchestrator:
    """åè°ƒå¢é‡æ›´æ–°çš„å®Œæ•´æµç¨‹"""
    
    async def run_incremental_update_cycle(self):
        """è¿è¡Œå¢é‡æ›´æ–°å‘¨æœŸ"""
        
        # 1. æ£€æµ‹å˜åŒ–
        print("ğŸ” æ£€æµ‹å†…å®¹å˜åŒ–...")
        detector = IntelligentChangeDetector()
        changes = await detector.detect_changes(self.get_monitored_urls())
        
        # 2. è¯„ä¼°ä¼˜å…ˆçº§
        print("ğŸ“Š è¯„ä¼°æ›´æ–°ä¼˜å…ˆçº§...")
        assessor = UpdatePriorityAssessor()
        update_plan = await assessor.prioritize_updates(changes)
        
        # 3. æ‰§è¡Œå¢é‡æ›´æ–°
        print("ğŸ”„ æ‰§è¡Œå¢é‡æ›´æ–°...")
        crawler = IncrementalCrawler()
        update_results = await crawler.perform_incremental_update(update_plan)
        
        # 4. ç‰ˆæœ¬å¯¹æ¯”å’Œè¿½è¸ª
        print("ğŸ“ ç‰ˆæœ¬å¯¹æ¯”...")
        comparer = VersionComparer()
        for update in update_results['successful_updates']:
            await comparer.track_content_evolution(
                update['url'],
                update['new_content']
            )
        
        # 5. è§£å†³å†²çª
        print("âš–ï¸ è§£å†³å†²çª...")
        if update_results.get('conflicts'):
            resolver = ConflictResolver()
            resolutions = await resolver.resolve_update_conflicts(
                update_results['conflicts']
            )
        
        # 6. æ›´æ–°çŸ¥è¯†åº“
        print("ğŸ’¾ æ›´æ–°çŸ¥è¯†åº“...")
        await self.update_knowledge_base(update_results)
        
        # 7. é€šçŸ¥ç›¸å…³æ›´æ–°
        print("ğŸ“¢ å‘é€æ›´æ–°é€šçŸ¥...")
        await self.notify_important_changes(update_results)
        
        return {
            'updated_pages': len(update_results['successful_updates']),
            'new_discoveries': len(update_results['new_discoveries']),
            'failed_updates': len(update_results['failed_updates']),
            'next_cycle': await self.calculate_next_cycle_time()
        }
```

## ğŸ“Š å¢é‡æ›´æ–°ç­–ç•¥

### 1. **åˆ†å±‚æ›´æ–°é¢‘ç‡**
```python
UPDATE_FREQUENCIES = {
    'critical': {
        'check_interval': '1 hour',
        'pages': ['processing-times', 'program-status', 'alerts']
    },
    'important': {
        'check_interval': '1 day',
        'pages': ['requirements', 'fees', 'forms']
    },
    'regular': {
        'check_interval': '1 week',
        'pages': ['guides', 'faqs', 'tools']
    },
    'low': {
        'check_interval': '1 month',
        'pages': ['about', 'contact', 'general-info']
    }
}
```

### 2. **æ™ºèƒ½æ›´æ–°è§¦å‘**
- RSSè®¢é˜…é€šçŸ¥
- APIå˜åŒ–æ¨é€
- ç”¨æˆ·æŸ¥è¯¢æ¿€å¢
- å…³è”é¡µé¢æ›´æ–°
- å®šæœŸå·¡æ£€

### 3. **èµ„æºä¼˜åŒ–**
- åªçˆ¬å–å˜åŒ–éƒ¨åˆ†
- æ‰¹é‡å¤„ç†ç›¸ä¼¼æ›´æ–°
- é¿å¼€é«˜å³°æ—¶æ®µ
- è‡ªåŠ¨é‡è¯•å¤±è´¥

## ğŸ¯ æ•ˆæœæŒ‡æ ‡

```python
{
    "æ›´æ–°å»¶è¿Ÿ": "<1å°æ—¶ï¼ˆå…³é”®ä¿¡æ¯ï¼‰",
    "èµ„æºèŠ‚çœ": "90%ï¼ˆåªæ›´æ–°å˜åŒ–ï¼‰",
    "å‡†ç¡®ç‡": "99%ï¼ˆå¤šé‡éªŒè¯ï¼‰",
    "è¦†ç›–ç‡": "100%ï¼ˆå…¨ç«™ç›‘æ§ï¼‰",
    "è‡ªåŠ¨åŒ–ç‡": "100%ï¼ˆæ— éœ€äººå·¥ï¼‰"
}
```

è¿™ä¸ªå¢é‡æ›´æ–°ç³»ç»Ÿç¡®ä¿çŸ¥è¯†åº“å§‹ç»ˆä¿æŒæœ€æ–°ï¼ŒåŒæ—¶æœ€å¤§åŒ–æ•ˆç‡å’Œèµ„æºåˆ©ç”¨ã€‚