# ğŸ•·ï¸ æ™ºèƒ½çˆ¬è™«ç³»ç»Ÿæ”¹è¿›è®¡åˆ’

## ğŸ“‹ é¡¹ç›®èƒŒæ™¯

å½“å‰çˆ¬è™«ç³»ç»Ÿåœ¨çˆ¬å–åŠ æ‹¿å¤§ç§»æ°‘å®˜ç½‘ï¼ˆcanada.caï¼‰æ—¶å­˜åœ¨è¦†ç›–ç‡ä½ã€å†…å®¹æå–ä¸å®Œæ•´ç­‰é—®é¢˜ï¼Œå¯¼è‡´çŸ¥è¯†åº“ä¿¡æ¯ä¸å…¨é¢ï¼Œå½±å“AIå›ç­”çš„å‡†ç¡®æ€§ã€‚

## ğŸ¯ æ”¹è¿›ç›®æ ‡

1. **å®Œæ•´è¦†ç›–å®˜ç½‘**ï¼šç¡®ä¿çˆ¬å–æ‰€æœ‰ç§»æ°‘ç›¸å…³é¡µé¢ï¼Œä¸é—æ¼é‡è¦ä¿¡æ¯
2. **ç»“æ„åŒ–æ•°æ®æå–**ï¼šæ­£ç¡®æå–è¡¨æ ¼ã€åˆ—è¡¨ã€PDFç­‰ç»“æ„åŒ–å†…å®¹
3. **æ™ºèƒ½å†…å®¹è¯†åˆ«**ï¼šå‡†ç¡®è¯†åˆ«ä¸åŒç±»å‹çš„ç§»æ°‘è·¯å¾„å’Œè¦æ±‚
4. **å¢é‡æ›´æ–°æ”¯æŒ**ï¼šé«˜æ•ˆæ›´æ–°å˜åŒ–çš„å†…å®¹ï¼Œé¿å…é‡å¤çˆ¬å–

## ğŸ”§ æŠ€æœ¯æ”¹è¿›æ–¹æ¡ˆ

### 1. å¢å¼ºçˆ¬å–ç­–ç•¥

#### 1.1 æ”¹è¿›URLä¼˜å…ˆçº§ç®—æ³•
```python
class SmartURLPrioritizer:
    """æ™ºèƒ½URLä¼˜å…ˆçº§æ’åºå™¨"""
    
    # é«˜ä¼˜å…ˆçº§è·¯å¾„æ¨¡å¼
    HIGH_PRIORITY_PATTERNS = [
        # ç§»æ°‘é¡¹ç›®
        r'/express-entry',
        r'/provincial-nominee',
        r'/start-up-visa',
        r'/self-employed',
        r'/family-sponsorship',
        r'/atlantic-immigration',
        r'/rural-northern',
        r'/caregivers',
        
        # å…³é”®æµç¨‹
        r'/how-to-apply',
        r'/eligibility',
        r'/requirements',
        r'/processing-times',
        r'/after-you-apply',
        r'/prepare-arrival',
        
        # å·¥å…·å’ŒæŒ‡å—
        r'/tools/',
        r'/guides/',
        r'/forms/',
        r'/fees/'
    ]
    
    def calculate_priority(self, url: str, parent_importance: float) -> float:
        """è®¡ç®—URLä¼˜å…ˆçº§åˆ†æ•°"""
        score = 0.0
        
        # 1. URLè·¯å¾„åŒ¹é…
        for pattern in self.HIGH_PRIORITY_PATTERNS:
            if re.search(pattern, url, re.I):
                score += 0.3
                
        # 2. ç»§æ‰¿çˆ¶é¡µé¢é‡è¦æ€§
        score += parent_importance * 0.3
        
        # 3. URLæ·±åº¦æƒ©ç½šï¼ˆä½†ä¸è¦å¤ªä¸¥æ ¼ï¼‰
        depth = len(urlparse(url).path.split('/'))
        score -= depth * 0.05
        
        # 4. æŸ¥è¯¢å‚æ•°å¥–åŠ±ï¼ˆå¯èƒ½æ˜¯ç­›é€‰æ¡ä»¶ï¼‰
        if '?' in url:
            score += 0.1
            
        return min(max(score, 0), 1.0)
```

#### 1.2 å®ç°ç«™ç‚¹åœ°å›¾ä¼˜å…ˆçˆ¬å–
```python
async def crawl_sitemap_first(self):
    """å…ˆçˆ¬å–ç«™ç‚¹åœ°å›¾è·å–æ‰€æœ‰URL"""
    sitemap_urls = [
        "https://www.canada.ca/sitemap.xml",
        "https://www.canada.ca/en/immigration-refugees-citizenship/sitemap.xml"
    ]
    
    all_urls = []
    for sitemap_url in sitemap_urls:
        urls = await self.parse_sitemap(sitemap_url)
        # è¿‡æ»¤ç§»æ°‘ç›¸å…³URL
        immigration_urls = [
            url for url in urls 
            if 'immigration' in url or 'visa' in url or 'permit' in url
        ]
        all_urls.extend(immigration_urls)
    
    return self.prioritize_urls(all_urls)
```

### 2. å¢å¼ºå†…å®¹æå–

#### 2.1 è¡¨æ ¼æ•°æ®æå–å™¨
```python
class TableExtractor:
    """è¡¨æ ¼æ•°æ®æå–å™¨"""
    
    async def extract_tables(self, page):
        """æå–é¡µé¢ä¸­çš„æ‰€æœ‰è¡¨æ ¼"""
        tables = await page.query_selector_all('table')
        extracted_tables = []
        
        for table in tables:
            # æå–è¡¨æ ¼æ ‡é¢˜
            caption = await table.query_selector('caption')
            title = await caption.inner_text() if caption else ""
            
            # æå–è¡¨å¤´
            headers = []
            header_cells = await table.query_selector_all('th')
            for cell in header_cells:
                headers.append(await cell.inner_text())
            
            # æå–æ•°æ®è¡Œ
            rows = []
            data_rows = await table.query_selector_all('tr')
            for row in data_rows:
                cells = await row.query_selector_all('td')
                if cells:
                    row_data = []
                    for cell in cells:
                        row_data.append(await cell.inner_text())
                    rows.append(row_data)
            
            extracted_tables.append({
                'title': title,
                'headers': headers,
                'data': rows,
                'markdown': self.table_to_markdown(headers, rows)
            })
            
        return extracted_tables
    
    def table_to_markdown(self, headers, rows):
        """å°†è¡¨æ ¼è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        if not headers:
            return ""
            
        # æ„å»ºMarkdownè¡¨æ ¼
        md = "| " + " | ".join(headers) + " |\n"
        md += "| " + " | ".join(["-" * len(h) for h in headers]) + " |\n"
        
        for row in rows:
            if len(row) == len(headers):
                md += "| " + " | ".join(row) + " |\n"
                
        return md
```

#### 2.2 PDFå†…å®¹æå–
```python
class PDFExtractor:
    """PDFæ–‡æ¡£æå–å™¨"""
    
    async def should_download_pdf(self, pdf_url: str, title: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸‹è½½PDF"""
        important_pdfs = [
            'guide', 'checklist', 'form', 'instruction',
            'requirement', 'eligibility', 'process'
        ]
        
        # æ£€æŸ¥URLæˆ–æ ‡é¢˜æ˜¯å¦åŒ…å«é‡è¦å…³é”®è¯
        url_lower = pdf_url.lower()
        title_lower = title.lower()
        
        for keyword in important_pdfs:
            if keyword in url_lower or keyword in title_lower:
                return True
                
        return False
    
    async def extract_pdf_content(self, pdf_url: str) -> Dict:
        """æå–PDFå†…å®¹"""
        try:
            # ä¸‹è½½PDF
            pdf_content = await self.download_pdf(pdf_url)
            
            # ä½¿ç”¨ PyPDF2 æˆ– pdfplumber æå–æ–‡æœ¬
            text = self.extract_text_from_pdf(pdf_content)
            
            # æå–è¡¨å•å­—æ®µï¼ˆå¦‚æœæ˜¯å¯å¡«å†™è¡¨å•ï¼‰
            form_fields = self.extract_form_fields(pdf_content)
            
            return {
                'url': pdf_url,
                'type': 'pdf',
                'text': text,
                'form_fields': form_fields,
                'summary': self.generate_pdf_summary(text)
            }
        except Exception as e:
            logger.error(f"PDF extraction failed: {e}")
            return None
```

#### 2.3 äº¤äº’å¼å†…å®¹å¤„ç†
```python
class InteractiveContentHandler:
    """å¤„ç†JavaScriptåŠ¨æ€å†…å®¹"""
    
    async def handle_accordions(self, page):
        """å±•å¼€æ‰€æœ‰æŠ˜å å†…å®¹"""
        # æŸ¥æ‰¾æ‰€æœ‰å¯å±•å¼€å…ƒç´ 
        accordions = await page.query_selector_all('[data-toggle="collapse"], .accordion-button')
        
        for accordion in accordions:
            try:
                await accordion.click()
                await page.wait_for_timeout(500)  # ç­‰å¾…åŠ¨ç”»å®Œæˆ
            except:
                pass
    
    async def handle_tabs(self, page):
        """åˆ‡æ¢æ‰€æœ‰æ ‡ç­¾é¡µè·å–å†…å®¹"""
        tabs = await page.query_selector_all('[role="tab"]')
        all_tab_content = []
        
        for tab in tabs:
            try:
                await tab.click()
                await page.wait_for_timeout(500)
                
                # è·å–å½“å‰æ ‡ç­¾å†…å®¹
                content = await page.evaluate('''
                    () => {
                        const activePanel = document.querySelector('[role="tabpanel"][aria-hidden="false"]');
                        return activePanel ? activePanel.innerText : '';
                    }
                ''')
                
                all_tab_content.append(content)
            except:
                pass
                
        return all_tab_content
```

### 3. ä¸åŒç§»æ°‘è·¯å¾„çš„ä¸“é—¨å¤„ç†

#### 3.1 ç§»æ°‘é¡¹ç›®è¯†åˆ«å™¨
```python
class ImmigrationProgramDetector:
    """è¯†åˆ«é¡µé¢æ‰€å±çš„ç§»æ°‘é¡¹ç›®"""
    
    PROGRAM_PATTERNS = {
        'express_entry': {
            'keywords': ['express entry', 'federal skilled worker', 'canadian experience class', 'federal skilled trades'],
            'urls': ['/express-entry/', '/ee/']
        },
        'pnp': {
            'keywords': ['provincial nominee', 'pnp', 'nomination certificate'],
            'urls': ['/provincial-nominees/', '/pnp/']
        },
        'start_up_visa': {
            'keywords': ['start-up visa', 'suv', 'letter of support', 'designated organization'],
            'urls': ['/start-visa/', '/suv/']
        },
        'family_sponsorship': {
            'keywords': ['sponsor', 'spouse', 'common-law', 'conjugal', 'dependent child', 'parent', 'grandparent'],
            'urls': ['/sponsor/', '/family/']
        },
        'caregiver': {
            'keywords': ['caregiver', 'home child care', 'home support worker'],
            'urls': ['/caregivers/']
        }
    }
    
    def detect_program(self, url: str, title: str, content: str) -> List[str]:
        """æ£€æµ‹é¡µé¢æ¶‰åŠçš„ç§»æ°‘é¡¹ç›®"""
        detected_programs = []
        
        url_lower = url.lower()
        title_lower = title.lower()
        content_lower = content.lower()[:1000]  # åªæ£€æŸ¥å‰1000å­—ç¬¦
        
        for program, patterns in self.PROGRAM_PATTERNS.items():
            # URLåŒ¹é…
            for url_pattern in patterns['urls']:
                if url_pattern in url_lower:
                    detected_programs.append(program)
                    break
                    
            # å…³é”®è¯åŒ¹é…
            for keyword in patterns['keywords']:
                if keyword in title_lower or keyword in content_lower:
                    detected_programs.append(program)
                    break
                    
        return list(set(detected_programs))
```

#### 3.2 ç‰¹å®šå†…å®¹å¢å¼ºæå–
```python
class ProgramSpecificExtractor:
    """é’ˆå¯¹ä¸åŒç§»æ°‘é¡¹ç›®çš„ç‰¹å®šå†…å®¹æå–"""
    
    async def extract_express_entry_content(self, page, content):
        """æå–Express Entryç‰¹å®šå†…å®¹"""
        extracted = {
            'program': 'express_entry',
            'crs_score_table': None,
            'noc_requirements': None,
            'language_requirements': None,
            'education_assessment': None
        }
        
        # æŸ¥æ‰¾CRSåˆ†æ•°è¡¨
        crs_tables = await self.find_tables_by_keywords(page, ['comprehensive ranking system', 'crs', 'points'])
        if crs_tables:
            extracted['crs_score_table'] = crs_tables[0]
            
        # æå–è¯­è¨€è¦æ±‚
        language_section = await self.find_section_by_heading(page, ['language', 'clb', 'ielts', 'celpip'])
        if language_section:
            extracted['language_requirements'] = language_section
            
        return extracted
    
    async def extract_pnp_content(self, page, content):
        """æå–PNPç‰¹å®šå†…å®¹"""
        extracted = {
            'program': 'pnp',
            'provinces': [],
            'streams': [],
            'requirements_by_province': {}
        }
        
        # è¯†åˆ«çœä»½
        provinces = ['alberta', 'british columbia', 'manitoba', 'ontario', 'quebec', 'saskatchewan']
        for province in provinces:
            if province in content.lower():
                extracted['provinces'].append(province)
                
                # æŸ¥æ‰¾è¯¥çœä»½çš„å…·ä½“è¦æ±‚
                province_section = await self.find_section_by_heading(page, [province])
                if province_section:
                    extracted['requirements_by_province'][province] = province_section
                    
        return extracted
```

### 4. æ™ºèƒ½å»é‡å’Œå¢é‡æ›´æ–°

#### 4.1 å†…å®¹æŒ‡çº¹ç”Ÿæˆ
```python
class ContentFingerprint:
    """ç”Ÿæˆå†…å®¹æŒ‡çº¹ç”¨äºå»é‡"""
    
    def generate_fingerprint(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹æŒ‡çº¹"""
        # 1. è§„èŒƒåŒ–æ–‡æœ¬
        normalized = self.normalize_text(content)
        
        # 2. æå–å…³é”®å¥å­
        key_sentences = self.extract_key_sentences(normalized)
        
        # 3. ç”Ÿæˆå“ˆå¸Œ
        content_hash = hashlib.sha256(
            ''.join(key_sentences).encode()
        ).hexdigest()
        
        return content_hash
    
    def normalize_text(self, text: str) -> str:
        """è§„èŒƒåŒ–æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s]', '', text)
        # è½¬å°å†™
        return text.lower().strip()
```

#### 4.2 å¢é‡æ›´æ–°æ£€æµ‹
```python
class IncrementalUpdateDetector:
    """æ£€æµ‹é¡µé¢æ˜¯å¦æœ‰æ›´æ–°"""
    
    def __init__(self, vector_store):
        self.vector_store = vector_store
        
    async def has_content_changed(self, url: str, new_content: str) -> bool:
        """æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰å®è´¨æ€§å˜åŒ–"""
        # è·å–å·²å­˜å‚¨çš„å†…å®¹
        existing = await self.vector_store.get_by_url(url)
        if not existing:
            return True
            
        # æ¯”è¾ƒå†…å®¹æŒ‡çº¹
        old_fingerprint = existing.get('fingerprint')
        new_fingerprint = ContentFingerprint().generate_fingerprint(new_content)
        
        if old_fingerprint != new_fingerprint:
            # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦åªæ˜¯å°æ”¹åŠ¨
            similarity = self.calculate_similarity(
                existing.get('content'), 
                new_content
            )
            
            # å¦‚æœç›¸ä¼¼åº¦ä½äº90%ï¼Œè®¤ä¸ºæœ‰å®è´¨æ€§æ›´æ–°
            return similarity < 0.9
            
        return False
```

### 5. çˆ¬è™«è°ƒåº¦ä¼˜åŒ–

#### 5.1 å¹¶è¡Œçˆ¬å–ç®¡ç†
```python
class CrawlerScheduler:
    """çˆ¬è™«è°ƒåº¦å™¨"""
    
    def __init__(self, max_concurrent: int = 10):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
    async def crawl_batch(self, urls: List[str]):
        """æ‰¹é‡çˆ¬å–URL"""
        tasks = []
        
        for url in urls:
            task = self.crawl_with_limit(url)
            tasks.append(task)
            
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¤±è´¥çš„URLï¼ŒåŠ å…¥é‡è¯•é˜Ÿåˆ—
        failed_urls = []
        for url, result in zip(urls, results):
            if isinstance(result, Exception):
                failed_urls.append(url)
                
        return results, failed_urls
    
    async def crawl_with_limit(self, url: str):
        """é™æµçˆ¬å–"""
        async with self.semaphore:
            return await self.crawl_single_url(url)
```

### 6. é…ç½®æ–‡ä»¶ä¼˜åŒ–

```yaml
# improved_immigration_crawler.yaml
crawler_config:
  # åŸºç¡€è®¾ç½®
  max_depth: 10  # é€‚ä¸­çš„æ·±åº¦
  max_pages: 5000  # è¶³å¤Ÿè¦†ç›–æ•´ä¸ªç½‘ç«™
  concurrent_requests: 20  # æé«˜å¹¶å‘
  
  # å†…å®¹æå–
  extract_tables: true
  extract_pdfs: true
  handle_javascript: true
  
  # æ™ºèƒ½çˆ¬å–
  use_sitemap: true
  smart_url_priority: true
  detect_programs: true
  
  # è´¨é‡æ§åˆ¶
  min_content_length: 100
  max_content_length: 50000
  importance_threshold: 0.2
  
  # ç‰¹å®šç¨‹åºé…ç½®
  program_specific:
    express_entry:
      priority_boost: 0.3
      extract_crs_table: true
      extract_noc_codes: true
    pnp:
      priority_boost: 0.3
      extract_provincial_requirements: true
    start_up_visa:
      priority_boost: 0.25
      extract_designated_orgs: true
      
  # æ’é™¤æ¨¡å¼ï¼ˆä¼˜åŒ–åï¼‰
  exclude_patterns:
    - '/fr/'  # æ³•è¯­é¡µé¢
    - '/news-releases/'  # æ–°é—»ç¨¿
    - '/media/'  # åª’ä½“èµ„æº
    - '/video/'  # è§†é¢‘
    - '/(contact|help|search)/'  # åŠŸèƒ½é¡µé¢
    
  # åŒ…å«æ¨¡å¼ï¼ˆæ–°å¢ï¼‰
  include_patterns:
    - '/services/immigrate'
    - '/immigration-refugees-citizenship'
    - '/en/immigration'
    - '/apply/'
    - '/tools/'
    - '/guides/'
```

## ğŸ“Š å®æ–½è®¡åˆ’

### ç¬¬ä¸€é˜¶æ®µï¼šæ ¸å¿ƒæ”¹è¿›ï¼ˆ1å‘¨ï¼‰
1. âœ… å®ç°æ™ºèƒ½URLä¼˜å…ˆçº§ç®—æ³•
2. âœ… æ·»åŠ è¡¨æ ¼æ•°æ®æå–å™¨
3. âœ… å®ç°åŸºç¡€çš„ç¨‹åºè¯†åˆ«å™¨

### ç¬¬äºŒé˜¶æ®µï¼šå†…å®¹å¢å¼ºï¼ˆ2å‘¨ï¼‰
1. âœ… PDFå†…å®¹æå–
2. âœ… JavaScriptåŠ¨æ€å†…å®¹å¤„ç†
3. âœ… ç‰¹å®šç¨‹åºå†…å®¹æå–å™¨

### ç¬¬ä¸‰é˜¶æ®µï¼šä¼˜åŒ–å’Œæµ‹è¯•ï¼ˆ1å‘¨ï¼‰
1. âœ… å†…å®¹å»é‡æœºåˆ¶
2. âœ… å¢é‡æ›´æ–°æ”¯æŒ
3. âœ… æ€§èƒ½ä¼˜åŒ–å’Œæµ‹è¯•

## ğŸ¯ é¢„æœŸæ•ˆæœ

1. **è¦†ç›–ç‡æå‡**ï¼šä»20é¡µæå‡åˆ°2000+é¡µï¼Œè¦†ç›–95%ä»¥ä¸Šçš„ç§»æ°‘ç›¸å…³å†…å®¹
2. **å†…å®¹è´¨é‡**ï¼šæ­£ç¡®æå–æ‰€æœ‰è¡¨æ ¼ã€è¦æ±‚åˆ—è¡¨ã€PDFæŒ‡å—
3. **æ›´æ–°æ•ˆç‡**ï¼šå¢é‡æ›´æ–°åªå¤„ç†å˜åŒ–å†…å®¹ï¼Œå‡å°‘90%çš„é‡å¤çˆ¬å–
4. **æŸ¥è¯¢å‡†ç¡®æ€§**ï¼šAIèƒ½å¤Ÿå‡†ç¡®å›ç­”å„ç§ç§»æ°‘è·¯å¾„çš„å…·ä½“è¦æ±‚

## ğŸ” ç›‘æ§æŒ‡æ ‡

```python
class CrawlerMetrics:
    """çˆ¬è™«æ€§èƒ½æŒ‡æ ‡"""
    
    def __init__(self):
        self.metrics = {
            'total_urls_discovered': 0,
            'total_pages_crawled': 0,
            'relevant_pages_found': 0,
            'tables_extracted': 0,
            'pdfs_processed': 0,
            'programs_detected': Counter(),
            'content_duplicates': 0,
            'crawl_speed': 0,  # pages per minute
            'error_rate': 0
        }
    
    def generate_report(self):
        """ç”Ÿæˆçˆ¬è™«æ•ˆæœæŠ¥å‘Š"""
        return {
            'coverage': {
                'total_pages': self.metrics['total_pages_crawled'],
                'relevant_pages': self.metrics['relevant_pages_found'],
                'relevance_rate': self.metrics['relevant_pages_found'] / max(self.metrics['total_pages_crawled'], 1)
            },
            'content_extraction': {
                'tables': self.metrics['tables_extracted'],
                'pdfs': self.metrics['pdfs_processed'],
                'programs': dict(self.metrics['programs_detected'])
            },
            'efficiency': {
                'speed': self.metrics['crawl_speed'],
                'duplicate_rate': self.metrics['content_duplicates'] / max(self.metrics['total_pages_crawled'], 1),
                'error_rate': self.metrics['error_rate']
            }
        }
```

---

é€šè¿‡è¿™äº›æ”¹è¿›ï¼Œçˆ¬è™«ç³»ç»Ÿå°†èƒ½å¤Ÿï¼š
1. å®Œæ•´çˆ¬å–åŠ æ‹¿å¤§ç§»æ°‘å®˜ç½‘çš„æ‰€æœ‰ç›¸å…³å†…å®¹
2. æ­£ç¡®æå–å„ç§æ ¼å¼çš„æ•°æ®ï¼ˆè¡¨æ ¼ã€PDFã€åŠ¨æ€å†…å®¹ï¼‰
3. æ™ºèƒ½è¯†åˆ«ä¸åŒçš„ç§»æ°‘é¡¹ç›®å’Œè·¯å¾„
4. é«˜æ•ˆè¿›è¡Œå¢é‡æ›´æ–°ï¼Œä¿æŒçŸ¥è¯†åº“çš„æ—¶æ•ˆæ€§

è¿™å°†æ˜¾è‘—æå‡AIåŠ©æ‰‹å›ç­”çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚