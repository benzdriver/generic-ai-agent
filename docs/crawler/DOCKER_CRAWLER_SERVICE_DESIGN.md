# ğŸ³ æ™ºèƒ½çˆ¬è™«å‘é‡åŒ–æœåŠ¡è®¾è®¡

## ğŸ“‹ æ¦‚è¿°

è®¾è®¡ä¸€ä¸ªç‹¬ç«‹çš„DockeræœåŠ¡ï¼Œæä¾›"URLåˆ—è¡¨ â†’ æ™ºèƒ½çˆ¬å– â†’ å‘é‡æ•°æ®åº“"çš„å®Œæ•´pipelineã€‚

## ğŸ” ç°æœ‰å¼€æºæ–¹æ¡ˆåˆ†æ

### 1. **Firecrawl** â­â­â­â­â­
- **GitHub**: https://github.com/mendableai/firecrawl
- **ç‰¹ç‚¹**: 
  - ä¸“ä¸ºLLMè®¾è®¡çš„çˆ¬è™«
  - è‡ªåŠ¨å¤„ç†JSæ¸²æŸ“
  - è¾“å‡ºMarkdownæ ¼å¼
  - æä¾›APIå’ŒSDK
- **ç¼ºç‚¹**: ä¸åŒ…å«å‘é‡åŒ–å’ŒçŸ¥è¯†åº“ç®¡ç†

### 2. **Crawlee** â­â­â­â­
- **GitHub**: https://github.com/apify/crawlee
- **ç‰¹ç‚¹**:
  - æ”¯æŒPlaywright/Puppeteer
  - è‡ªåŠ¨é‡è¯•å’Œé”™è¯¯å¤„ç†
  - åˆ†å¸ƒå¼çˆ¬å–
- **ç¼ºç‚¹**: éœ€è¦è‡ªå·±å®ç°å‘é‡åŒ–

### 3. **LlamaIndex Web Reader** â­â­â­
- **GitHub**: https://github.com/run-llama/llama_index
- **ç‰¹ç‚¹**:
  - é›†æˆå‘é‡å­˜å‚¨
  - æ”¯æŒå¤šç§æ•°æ®æº
  - LLMå‹å¥½
- **ç¼ºç‚¹**: çˆ¬è™«åŠŸèƒ½è¾ƒç®€å•

### 4. **Embedbase** â­â­â­
- **GitHub**: https://github.com/different-ai/embedbase
- **ç‰¹ç‚¹**:
  - ä¸“æ³¨äºå‘é‡åŒ–å­˜å‚¨
  - æ”¯æŒå¤šç§åµŒå…¥æ¨¡å‹
  - REST API
- **ç¼ºç‚¹**: çˆ¬è™«åŠŸèƒ½æœ‰é™

### 5. **Danswer** â­â­â­â­
- **GitHub**: https://github.com/danswer-ai/danswer
- **ç‰¹ç‚¹**:
  - ä¼ä¸šçº§çŸ¥è¯†åº“
  - æ”¯æŒå¤šæ•°æ®æº
  - åŒ…å«é—®ç­”ç³»ç»Ÿ
- **ç¼ºç‚¹**: è¾ƒé‡ï¼Œä¸å¤Ÿçµæ´»

## ğŸ—ï¸ æ¨èæ¶æ„ï¼šç»„åˆæ–¹æ¡ˆ

```yaml
# docker-compose.yml
version: '3.8'

services:
  # 1. æ™ºèƒ½çˆ¬è™«æœåŠ¡ï¼ˆåŸºäºFirecrawlæ”¹é€ ï¼‰
  smart-crawler:
    build: ./smart-crawler
    environment:
      - CRAWLER_MODE=intelligent
      - LLM_PROVIDER=openai
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    ports:
      - "3001:3001"
    depends_on:
      - redis
      - postgres

  # 2. å‘é‡åŒ–æœåŠ¡
  vectorizer:
    build: ./vectorizer
    environment:
      - EMBEDDING_MODEL=text-embedding-3-small
      - BATCH_SIZE=100
    depends_on:
      - qdrant

  # 3. å‘é‡æ•°æ®åº“
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage

  # 4. ä»»åŠ¡é˜Ÿåˆ—
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  # 5. å…ƒæ•°æ®å­˜å‚¨
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=crawler_db
      - POSTGRES_USER=crawler
      - POSTGRES_PASSWORD=secret
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # 6. APIç½‘å…³
  api-gateway:
    build: ./api-gateway
    ports:
      - "8080:8080"
    depends_on:
      - smart-crawler
      - vectorizer

volumes:
  qdrant_storage:
  postgres_data:
```

## ğŸ”§ æœåŠ¡è®¾è®¡

### 1. **æ ¸å¿ƒAPIè®¾è®¡**

```python
# api_gateway/main.py
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional, Dict

app = FastAPI(title="Intelligent Crawler Service")

class CrawlRequest(BaseModel):
    urls: List[str]
    config: Optional[Dict] = {
        "max_depth": 3,
        "smart_extraction": True,
        "extract_tables": True,
        "handle_pdfs": True,
        "language": "en"
    }
    vector_config: Optional[Dict] = {
        "collection_name": "default",
        "embedding_model": "text-embedding-3-small",
        "chunk_size": 1000,
        "chunk_overlap": 200
    }

class CrawlJob(BaseModel):
    job_id: str
    status: str
    progress: float
    results_preview: Optional[List[Dict]]

@app.post("/crawl", response_model=CrawlJob)
async def create_crawl_job(request: CrawlRequest, background_tasks: BackgroundTasks):
    """åˆ›å»ºçˆ¬å–ä»»åŠ¡"""
    job_id = generate_job_id()
    
    # å¼‚æ­¥æ‰§è¡Œçˆ¬å–
    background_tasks.add_task(
        crawl_and_vectorize,
        job_id=job_id,
        urls=request.urls,
        config=request.config,
        vector_config=request.vector_config
    )
    
    return CrawlJob(
        job_id=job_id,
        status="queued",
        progress=0.0
    )

@app.get("/job/{job_id}")
async def get_job_status(job_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    return get_job_details(job_id)

@app.post("/search")
async def search_knowledge(query: str, collection: str = "default"):
    """æœç´¢çŸ¥è¯†åº“"""
    results = await vector_search(query, collection)
    return results

@app.post("/update/{collection}")
async def incremental_update(collection: str):
    """è§¦å‘å¢é‡æ›´æ–°"""
    return await trigger_incremental_update(collection)
```

### 2. **æ™ºèƒ½çˆ¬è™«æœåŠ¡**

```python
# smart_crawler/crawler.py
class IntelligentCrawlerService:
    """åŸºäºFirecrawlæ”¹é€ çš„æ™ºèƒ½çˆ¬è™«"""
    
    def __init__(self):
        self.llm = self._init_llm()
        self.browser_pool = BrowserPool(size=10)
        
    async def crawl_urls(self, urls: List[str], config: Dict) -> List[Dict]:
        """æ™ºèƒ½çˆ¬å–URLåˆ—è¡¨"""
        
        results = []
        
        for url in urls:
            # 1. æ™ºèƒ½åˆ¤æ–­é¡µé¢ä»·å€¼
            if config.get('smart_extraction'):
                page_value = await self._assess_page_value(url)
                if page_value < config.get('min_value', 0.5):
                    continue
            
            # 2. çˆ¬å–å†…å®¹
            content = await self._crawl_page(url, config)
            
            # 3. æ™ºèƒ½æå–
            if config.get('extract_tables'):
                content['tables'] = await self._extract_tables(content['html'])
                
            if config.get('handle_pdfs') and content.get('pdf_links'):
                content['pdfs'] = await self._process_pdfs(content['pdf_links'])
            
            # 4. ç”Ÿæˆç»“æ„åŒ–è¾“å‡º
            structured = await self._structure_content(content)
            
            results.append({
                'url': url,
                'title': content['title'],
                'markdown': content['markdown'],
                'structured_data': structured,
                'metadata': {
                    'crawled_at': datetime.now(),
                    'page_value': page_value,
                    'extraction_config': config
                }
            })
            
        return results
    
    async def _assess_page_value(self, url: str) -> float:
        """AIè¯„ä¼°é¡µé¢ä»·å€¼"""
        # å¿«é€Ÿé¢„è§ˆé¡µé¢
        preview = await self._quick_preview(url)
        
        assessment = await self.llm.assess(f"""
        Evaluate this page's value for knowledge extraction:
        URL: {url}
        Title: {preview['title']}
        Preview: {preview['text'][:500]}
        
        Return a score 0-1.
        """)
        
        return assessment['score']
```

### 3. **å‘é‡åŒ–æœåŠ¡**

```python
# vectorizer/service.py
class VectorizerService:
    """æ–‡æ¡£å‘é‡åŒ–æœåŠ¡"""
    
    def __init__(self):
        self.embedder = EmbeddingModel()
        self.chunker = TextChunker()
        self.vector_store = QdrantClient()
        
    async def vectorize_documents(self, documents: List[Dict], config: Dict):
        """å‘é‡åŒ–æ–‡æ¡£"""
        
        collection = config['collection_name']
        
        # ç¡®ä¿é›†åˆå­˜åœ¨
        await self._ensure_collection(collection)
        
        all_chunks = []
        
        for doc in documents:
            # 1. æ™ºèƒ½åˆ†å—
            chunks = self.chunker.chunk(
                text=doc['markdown'],
                chunk_size=config['chunk_size'],
                overlap=config['chunk_overlap'],
                preserve_structure=True  # ä¿æŒæ®µè½/ç« èŠ‚ç»“æ„
            )
            
            # 2. ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®
            for chunk in chunks:
                chunk['metadata'] = {
                    'source_url': doc['url'],
                    'title': doc['title'],
                    'chunk_index': chunk['index'],
                    'total_chunks': len(chunks),
                    **doc.get('structured_data', {})
                }
                all_chunks.append(chunk)
        
        # 3. æ‰¹é‡å‘é‡åŒ–
        embeddings = await self._batch_embed(
            [chunk['text'] for chunk in all_chunks]
        )
        
        # 4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        await self._store_vectors(collection, all_chunks, embeddings)
        
        return {
            'collection': collection,
            'documents_processed': len(documents),
            'chunks_created': len(all_chunks)
        }
```

### 4. **å¢é‡æ›´æ–°æœåŠ¡**

```python
# updater/service.py
class IncrementalUpdateService:
    """å¢é‡æ›´æ–°æœåŠ¡"""
    
    def __init__(self):
        self.change_detector = ChangeDetector()
        self.scheduler = UpdateScheduler()
        
    async def setup_monitoring(self, collection: str):
        """è®¾ç½®ç›‘æ§"""
        
        # è·å–é›†åˆä¸­çš„æ‰€æœ‰URL
        urls = await self._get_collection_urls(collection)
        
        # åˆ›å»ºç›‘æ§ä»»åŠ¡
        for url in urls:
            await self.scheduler.schedule_monitoring(
                url=url,
                frequency=self._determine_frequency(url),
                callback=self._handle_change
            )
    
    async def _handle_change(self, url: str, change_type: str):
        """å¤„ç†æ£€æµ‹åˆ°çš„å˜åŒ–"""
        
        if change_type == 'content_update':
            # é‡æ–°çˆ¬å–å’Œå‘é‡åŒ–
            await self._update_document(url)
            
        elif change_type == 'page_removed':
            # ä»å‘é‡åº“åˆ é™¤
            await self._remove_document(url)
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### 1. **åŸºç¡€ä½¿ç”¨**

```python
import requests

# åˆ›å»ºçˆ¬å–ä»»åŠ¡
response = requests.post('http://localhost:8080/crawl', json={
    'urls': [
        'https://example.com/docs',
        'https://example.com/guides'
    ],
    'config': {
        'max_depth': 2,
        'smart_extraction': True
    },
    'vector_config': {
        'collection_name': 'my_knowledge_base'
    }
})

job = response.json()
print(f"Job created: {job['job_id']}")

# æ£€æŸ¥çŠ¶æ€
status = requests.get(f'http://localhost:8080/job/{job["job_id"]}').json()
print(f"Progress: {status['progress']}%")

# æœç´¢çŸ¥è¯†åº“
results = requests.post('http://localhost:8080/search', json={
    'query': 'How to get started?',
    'collection': 'my_knowledge_base'
}).json()
```

### 2. **SDKä½¿ç”¨**

```python
from intelligent_crawler import CrawlerClient

client = CrawlerClient('http://localhost:8080')

# çˆ¬å–å¹¶å‘é‡åŒ–
job = client.crawl(
    urls=['https://docs.example.com'],
    max_depth=3,
    collection='docs'
)

# ç­‰å¾…å®Œæˆ
job.wait_until_complete()

# æœç´¢
results = client.search('installation guide', collection='docs')
```

## ğŸ”§ é…ç½®é€‰é¡¹

```yaml
# config.yaml
crawler:
  concurrent_requests: 10
  timeout: 30
  user_agent: "IntelligentCrawler/1.0"
  respect_robots: true
  
extraction:
  use_ai: true
  extract_tables: true
  extract_images: false
  handle_pdfs: true
  pdf_max_size: 10MB
  
vectorization:
  model: "text-embedding-3-small"
  chunk_size: 1000
  chunk_overlap: 200
  batch_size: 100
  
storage:
  vector_db: "qdrant"
  metadata_db: "postgres"
  cache: "redis"
  
monitoring:
  enable_incremental: true
  check_frequency: "1h"
  change_threshold: 0.05
```

## ğŸ¯ ä¼˜åŠ¿

1. **æ¨¡å—åŒ–è®¾è®¡**: æ¯ä¸ªç»„ä»¶ç‹¬ç«‹ï¼Œæ˜“äºæ‰©å±•
2. **æ™ºèƒ½åŒ–**: AIé©±åŠ¨çš„å†…å®¹è¯„ä¼°å’Œæå–
3. **å¯æ‰©å±•**: æ”¯æŒæ°´å¹³æ‰©å±•
4. **å¤šç§Ÿæˆ·**: æ”¯æŒå¤šä¸ªçŸ¥è¯†åº“é›†åˆ
5. **å¢é‡æ›´æ–°**: è‡ªåŠ¨ä¿æŒå†…å®¹æœ€æ–°
6. **æ ‡å‡†API**: RESTfulæ¥å£ï¼Œæ˜“äºé›†æˆ

## ğŸ”¨ å¿«é€Ÿå¯åŠ¨

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/yourorg/intelligent-crawler-service

# é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ·»åŠ  API keys

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# æµ‹è¯•API
curl http://localhost:8080/health
```

è¿™ä¸ªè®¾è®¡ç»“åˆäº†ç°æœ‰å¼€æºé¡¹ç›®çš„ä¼˜ç‚¹ï¼Œå¹¶æ·»åŠ äº†æ™ºèƒ½åŒ–åŠŸèƒ½ï¼Œå¯ä»¥ä½œä¸ºé€šç”¨çš„çˆ¬è™«å‘é‡åŒ–æœåŠ¡ç”¨äºå¤šä¸ªé¡¹ç›®ã€‚