#!/usr/bin/env python3
# src/scripts/crawl_immigration_sites.py

"""
ç§»æ°‘ç½‘ç«™çˆ¬è™«ï¼šå®šæœŸçˆ¬å–æŒ‡å®šçš„ç§»æ°‘å®˜æ–¹ç½‘ç«™å¹¶æ›´æ–°çŸ¥è¯†åº“
"""

import sys
import os
import requests
import time
import json
import logging
from pathlib import Path
from datetime import datetime
import hashlib
import uuid

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# å°è¯•å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from knowledge_ingestion.doc_parser import parse_html_content
    from vector_engine.embedding_router import get_embedding
    from qdrant_client import QdrantClient
    from config.env_manager import init_config
    from vector_engine.qdrant_client import DOCUMENT_COLLECTION
except ImportError as e:
    print(f"å¯¼å…¥é¡¹ç›®æ¨¡å—å¤±è´¥: {str(e)}")
    print("è¯·ç¡®ä¿æ‚¨åœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"{project_root}/logs/crawler_{datetime.now().strftime('%Y%m%d')}.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åˆ›å»ºlogsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
Path(f"{project_root}/logs").mkdir(exist_ok=True)

# åˆ›å»ºç¼“å­˜ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
CACHE_DIR = Path(f"{project_root}/cache/crawled_pages")
CACHE_DIR.mkdir(parents=True, exist_ok=True)

# åˆå§‹åŒ–é…ç½®
config = init_config()
qdrant_config = config['qdrant']

# é»˜è®¤çˆ¬å–çš„ç§»æ°‘å®˜æ–¹ç½‘ç«™åˆ—è¡¨
DEFAULT_SITES = [
    {
        "url": "https://www.canada.ca/en/immigration-refugees-citizenship/services/immigrate-canada.html",
        "name": "IRCC_Immigrate",
        "domain": "canada_immigration",
        "lang": "en"
    },
    {
        "url": "https://www.canada.ca/en/immigration-refugees-citizenship/services/study-canada.html",
        "name": "IRCC_Study",
        "domain": "canada_immigration",
        "lang": "en"
    },
    {
        "url": "https://www.canada.ca/en/immigration-refugees-citizenship/services/work-canada.html",
        "name": "IRCC_Work",
        "domain": "canada_immigration",
        "lang": "en"
    }
]

def get_client() -> QdrantClient:
    """è·å–Qdrantå®¢æˆ·ç«¯"""
    if qdrant_config['is_cloud']:
        return QdrantClient(
            url=qdrant_config['url'],
            api_key=qdrant_config['api_key']
        )
    return QdrantClient(url=qdrant_config['url'])

def upsert_documents_to_collection(paragraphs: list, metadata: dict = None):
    """ä¸Šä¼ æ–‡æ¡£åˆ°æŒ‡å®šçš„å‘é‡æ•°æ®åº“é›†åˆ"""
    client = get_client()
    points = []
    
    for paragraph in paragraphs:
        embedding = get_embedding(paragraph)
        point_id = str(uuid.uuid4())
        payload = {
            "text": paragraph,
        }
        if metadata:
            payload.update(metadata)

        points.append({
            "id": point_id,
            "vector": embedding,
            "payload": payload
        })

    client.upsert(
        collection_name=DOCUMENT_COLLECTION,
        points=points
    )
    logger.info(f"æˆåŠŸå°† {len(points)} ä¸ªå‘é‡ç‚¹å†™å…¥ {DOCUMENT_COLLECTION} é›†åˆ")

def load_site_list(file_path=None):
    """åŠ è½½è¦çˆ¬å–çš„ç½‘ç«™åˆ—è¡¨"""
    if file_path and Path(file_path).exists():
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½ç½‘ç«™åˆ—è¡¨æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    logger.info("ä½¿ç”¨é»˜è®¤ç½‘ç«™åˆ—è¡¨")
    return DEFAULT_SITES

def get_page_hash(url, content):
    """è®¡ç®—é¡µé¢å†…å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹å˜åŒ–"""
    return hashlib.md5((url + content).encode('utf-8')).hexdigest()

def has_page_changed(url, content):
    """æ£€æŸ¥é¡µé¢å†…å®¹æ˜¯å¦å·²æ›´æ”¹"""
    page_hash = get_page_hash(url, content)
    hash_file = CACHE_DIR / f"{hashlib.md5(url.encode('utf-8')).hexdigest()}.hash"
    
    if hash_file.exists():
        with open(hash_file, 'r') as f:
            old_hash = f.read().strip()
            if old_hash == page_hash:
                return False
    
    # ä¿å­˜æ–°çš„å“ˆå¸Œå€¼
    with open(hash_file, 'w') as f:
        f.write(page_hash)
    
    return True

def crawl_page(site):
    """çˆ¬å–å•ä¸ªé¡µé¢å¹¶å¤„ç†å†…å®¹"""
    url = site["url"]
    name = site["name"]
    domain = site.get("domain", "immigration")
    lang = site.get("lang", "en")
    
    logger.info(f"çˆ¬å–é¡µé¢: {url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        content = response.text
        
        # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å˜åŒ–
        if not has_page_changed(url, content):
            logger.info(f"é¡µé¢æœªå˜åŒ–ï¼Œè·³è¿‡å¤„ç†: {url}")
            return False
        
        # è§£æHTMLå†…å®¹
        logger.info(f"å¤„ç†é¡µé¢å†…å®¹: {url}")
        chunks = parse_html_content(content)
        
        if not chunks:
            logger.warning(f"æœªä»é¡µé¢æå–åˆ°æœ‰æ•ˆå†…å®¹: {url}")
            return False
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            "source": name,
            "url": url,
            "domain": domain,
            "language": lang,
            "crawled_at": datetime.now().isoformat()
        }
        
        # å°†å†…å®¹å†™å…¥å‘é‡æ•°æ®åº“çš„documentsé›†åˆ
        logger.info(f"æå–äº† {len(chunks)} ä¸ªæ®µè½ï¼Œå†™å…¥documentsé›†åˆ...")
        upsert_documents_to_collection(chunks, metadata)
        
        logger.info(f"æˆåŠŸå¤„ç†é¡µé¢: {url}")
        return True
        
    except Exception as e:
        logger.error(f"çˆ¬å–æˆ–å¤„ç†é¡µé¢å¤±è´¥ {url}: {str(e)}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸ•¸ï¸ ç§»æ°‘ç½‘ç«™çˆ¬è™«")
    print("=" * 70)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    import argparse
    parser = argparse.ArgumentParser(description='çˆ¬å–ç§»æ°‘å®˜æ–¹ç½‘ç«™å¹¶æ›´æ–°çŸ¥è¯†åº“')
    parser.add_argument('--sites', help='ç½‘ç«™åˆ—è¡¨JSONæ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()
    
    # åŠ è½½ç½‘ç«™åˆ—è¡¨
    sites = load_site_list(args.sites)
    
    if not sites:
        logger.error("æ²¡æœ‰è¦çˆ¬å–çš„ç½‘ç«™")
        return
    
    logger.info(f"å¼€å§‹çˆ¬å– {len(sites)} ä¸ªç½‘ç«™...")
    
    # çˆ¬å–æ¯ä¸ªç½‘ç«™
    success_count = 0
    for site in sites:
        if crawl_page(site):
            success_count += 1
        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        time.sleep(2)
    
    logger.info(f"çˆ¬å–å®Œæˆã€‚æˆåŠŸ: {success_count}/{len(sites)}")
    print(f"\nâœ… çˆ¬å–å®Œæˆã€‚æˆåŠŸå¤„ç†äº† {success_count}/{len(sites)} ä¸ªç½‘ç«™")

if __name__ == "__main__":
    main() 