# Placeholder for conflict_detector.py 

"""
å†²çªæ£€æµ‹å™¨ï¼šæ£€æµ‹å’Œåˆ†æçŸ¥è¯†ç‚¹ä¹‹é—´çš„å†²çª

åŠŸèƒ½ï¼š
1. è¯­ä¹‰ç›¸ä¼¼åº¦æ£€æµ‹
2. æ—¶é—´ç‰ˆæœ¬æ¯”è¾ƒ
3. å†…å®¹çŸ›ç›¾åˆ†æï¼ˆé€šè¿‡ LLMï¼‰
"""

import json
import datetime
from typing import List, Dict, Tuple
import numpy as np
from vector_engine.embedding_router import get_embedding
from vector_engine.qdrant_client import get_client, DOCUMENT_COLLECTION
from config.env_manager import init_config
from llm.factory import LLMFactory

# åˆå§‹åŒ–é…ç½®
config = init_config()
SIMILARITY_THRESHOLD = 0.92  # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼

# è·å–LLMå®ä¾‹
llm = LLMFactory.get_llm()

def analyze_conflict(text1: str, text2: str) -> Dict[str, any]:
    """åˆ†æä¸¤ä¸ªçŸ¥è¯†ç‚¹ä¹‹é—´çš„å†²çª
    
    Args:
        text1: ç¬¬ä¸€ä¸ªçŸ¥è¯†ç‚¹
        text2: ç¬¬äºŒä¸ªçŸ¥è¯†ç‚¹
        
    Returns:
        Dict: åŒ…å«å†²çªåˆ†æç»“æœçš„å­—å…¸
    """
    prompt = f"""è¯·åˆ†æä»¥ä¸‹ä¸¤ä¸ªçŸ¥è¯†ç‚¹æ˜¯å¦å­˜åœ¨å†²çªï¼Œå¹¶ç»™å‡ºè¯¦ç»†åˆ†æï¼š

çŸ¥è¯†ç‚¹1ï¼š
{text1}

çŸ¥è¯†ç‚¹2ï¼š
{text2}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢åˆ†æï¼š
1. æ˜¯å¦å­˜åœ¨ç›´æ¥å†²çª
2. æ˜¯å¦å­˜åœ¨æ½œåœ¨å†²çª
3. å†²çªçš„å…·ä½“å†…å®¹
4. å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- has_conflict: æ˜¯å¦å­˜åœ¨å†²çªï¼ˆtrue/falseï¼‰
- conflict_type: å†²çªç±»å‹ï¼ˆ"direct"/"potential"/"none"ï¼‰
- details: å…·ä½“å†²çªå†…å®¹
- solution: å»ºè®®çš„è§£å†³æ–¹æ¡ˆ
"""
    
    try:
        response = llm.generate(prompt)
        result = json.loads(response)
        return result
    except json.JSONDecodeError:
        return {
            "has_conflict": True,  # ä¿å®ˆèµ·è§ï¼Œè§£æå¤±è´¥æ—¶è®¤ä¸ºå¯èƒ½å­˜åœ¨å†²çª
            "conflict_type": "potential",
            "details": response,
            "solution": "éœ€è¦äººå·¥å®¡æ ¸ï¼ˆJSONè§£æå¤±è´¥ï¼‰"
        }

def check_content_conflict(text1: str, text2: str) -> Tuple[bool, str]:
    """ä½¿ç”¨LLMæ£€æŸ¥ä¸¤æ®µæ–‡æœ¬æ˜¯å¦å­˜åœ¨å†²çª
    
    Args:
        text1: ç¬¬ä¸€æ®µæ–‡æœ¬
        text2: ç¬¬äºŒæ®µæ–‡æœ¬
        
    Returns:
        Tuple[bool, str]: (æ˜¯å¦å­˜åœ¨å†²çª, åŸå› )
    """
    llm = LLMFactory.get_llm()
    
    # æ„å»ºæç¤ºè¯
    prompt = f"""è¯·åˆ†æä»¥ä¸‹ä¸¤æ®µæ–‡æœ¬æ˜¯å¦å­˜åœ¨å†²çªæˆ–çŸ›ç›¾ï¼š

æ–‡æœ¬1ï¼š
{text1}

æ–‡æœ¬2ï¼š
{text2}

è¯·ä»…è¿”å› "æœ‰å†²çª" æˆ– "æ— å†²çª"ï¼Œä»¥åŠä¸€å¥è¯è¯´æ˜åŸå› ã€‚
"""
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å—å¤„ç†
    if llm.should_use_chunks([text1, text2]):
        response = llm.generate_with_chunks([text1, text2], **{"prompt": prompt})
    else:
        response = llm.generate(prompt)
    
    # è§£æå“åº”
    has_conflict = "æœ‰å†²çª" in response
    reason = response.split("ï¼Œ", 1)[1] if "ï¼Œ" in response else ""
    
    return has_conflict, reason

def cosine_similarity(v1: List[float], v2: List[float]) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    v1_array = np.array(v1)
    v2_array = np.array(v2)
    return np.dot(v1_array, v2_array) / (np.linalg.norm(v1_array) * np.linalg.norm(v2_array))

def fetch_recent_documents(days: int = 30) -> List[Dict]:
    """è·å–æœ€è¿‘Nå¤©çš„çŸ¥è¯†æ¡ç›®"""
    client = get_client()
    cutoff = datetime.datetime.utcnow() - datetime.timedelta(days=days)
    
    points = client.scroll(
        collection_name=DOCUMENT_COLLECTION,
        scroll_filter={
            "must": [
                {
                    "key": "created_at",
                    "range": {
                        "gte": cutoff.isoformat()
                    }
                }
            ]
        },
        limit=1000,
        with_payload=True,
        with_vectors=True
    )[0]
    
    return [
        {
            "id": point.id,
            "vector": point.vector,
            "text": point.payload.get("content", ""),  # ä½¿ç”¨ content å­—æ®µ
            "created_at": point.payload.get("created_at")
        }
        for point in points
    ]

def detect_conflicts(knowledge_points: List[Dict] = None) -> List[Tuple[Dict, Dict, Dict]]:
    """æ£€æµ‹çŸ¥è¯†åº“ä¸­çš„å†²çª
    
    Args:
        knowledge_points: å¯é€‰çš„çŸ¥è¯†ç‚¹åˆ—è¡¨ï¼Œå¦‚æœä¸æä¾›åˆ™è·å–æœ€è¿‘çš„æ–‡æ¡£
        
    Returns:
        List[Tuple]: åŒ…å«å†²çªæ–‡æ¡£å¯¹å’Œåˆ†æç»“æœçš„åˆ—è¡¨
    """
    if knowledge_points is None:
        knowledge_points = fetch_recent_documents()
    
    conflicts = []
    for i, doc1 in enumerate(knowledge_points):
        for doc2 in knowledge_points[i+1:]:
            # 1. æ£€æŸ¥è¯­ä¹‰ç›¸ä¼¼åº¦
            similarity = cosine_similarity(doc1["vector"], doc2["vector"])
            
            if similarity > SIMILARITY_THRESHOLD:
                # 2. ä½¿ç”¨ LLM æ£€æŸ¥å†…å®¹å†²çª
                has_conflict, reason = check_content_conflict(doc1["text"], doc2["text"])
                
                if has_conflict:
                    # 3. è¯¦ç»†åˆ†æå†²çª
                    analysis = analyze_conflict(doc1["text"], doc2["text"])
                    analysis["similarity"] = similarity
                    analysis["initial_reason"] = reason
                    
                    conflicts.append((doc1, doc2, analysis))
    
    return conflicts

def resolve_conflicts(conflicts: List[Tuple[Dict, Dict, Dict]]) -> None:
    """è§£å†³æ£€æµ‹åˆ°çš„å†²çª
    
    ç­–ç•¥ï¼š
    1. å¯¹äºç›´æ¥å†²çªï¼šä¿ç•™æœ€æ–°ç‰ˆæœ¬ï¼Œå°†æ—§ç‰ˆæœ¬æ ‡è®°ä¸ºå·²å¼ƒç”¨
    2. å¯¹äºæ½œåœ¨å†²çªï¼šæ ‡è®°éœ€è¦äººå·¥å®¡æ ¸
    """
    client = get_client()
    
    for doc1, doc2, analysis in conflicts:
        time1 = datetime.datetime.fromisoformat(doc1["created_at"])
        time2 = datetime.datetime.fromisoformat(doc2["created_at"])
        
        if analysis["conflict_type"] == "direct":
            # ç›´æ¥å†²çªï¼šæ ‡è®°è¾ƒæ—§çš„ç‰ˆæœ¬ä¸ºå¼ƒç”¨
            obsolete_doc = doc1 if time1 < time2 else doc2
            client.update_points(
                collection_name=DOCUMENT_COLLECTION,
                points_selector={"points": [obsolete_doc["id"]]},
                payload={
                    "status": "obsolete",
                    "obsolete_reason": analysis["details"],
                    "obsolete_at": datetime.datetime.utcnow().isoformat()
                }
            )
            print(f"âœ… å·²å°†æ–‡æ¡£ {obsolete_doc['id']} æ ‡è®°ä¸ºå¼ƒç”¨")
        else:
            # æ½œåœ¨å†²çªï¼šæ ‡è®°ä¸¤ä¸ªæ–‡æ¡£éƒ½éœ€è¦å®¡æ ¸
            for doc in [doc1, doc2]:
                client.update_points(
                    collection_name=DOCUMENT_COLLECTION,
                    points_selector={"points": [doc["id"]]},
                    payload={
                        "status": "needs_review",
                        "review_reason": analysis["details"],
                        "flagged_at": datetime.datetime.utcnow().isoformat()
                    }
                )
            print(f"âš ï¸ å·²å°†æ–‡æ¡£ {doc1['id']} å’Œ {doc2['id']} æ ‡è®°ä¸ºéœ€è¦å®¡æ ¸")

if __name__ == "__main__":
    print("ğŸ” å¼€å§‹æ£€æµ‹çŸ¥è¯†åº“å†²çª...")
    conflicts = detect_conflicts()
    
    if conflicts:
        print(f"å‘ç° {len(conflicts)} å¯¹æ½œåœ¨å†²çªï¼š")
        for doc1, doc2, analysis in conflicts:
            print(f"\nç›¸ä¼¼åº¦ï¼š{analysis['similarity']:.2f}")
            print(f"å†²çªç±»å‹ï¼š{analysis['conflict_type']}")
            print(f"æ–‡æ¡£1ï¼š{doc1['text'][:100]}...")
            print(f"æ–‡æ¡£2ï¼š{doc2['text'][:100]}...")
            print(f"åˆ†æï¼š{analysis['details']}")
            print(f"å»ºè®®è§£å†³æ–¹æ¡ˆï¼š{analysis['solution']}")
        
        resolve_conflicts(conflicts)
    else:
        print("âœ… æœªå‘ç°å†²çª") 